{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "166a37a3",
   "metadata": {},
   "source": [
    "**Colab Execution:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tasarorcun/data-science-playbook/blob/main/04-genai-and-agents/001_working_with_openai_api.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb21ce8",
   "metadata": {},
   "source": [
    "# 1. Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388b15db",
   "metadata": {},
   "source": [
    "**Project:** Building a Stateful Clinical Conversational Engine (From API Calls to Agents)<br>\n",
    "**Author:** Orcun Tasar  |  **Role:** Senior Clinical Data Science Consultant<br>\n",
    "**Domain:** Healthcare · GenAI · Pharmacovigilance<br>\n",
    "**Tech Stack:** OpenAI API (v1.x), Python 3.10+, spaCy (NLP), Jupyter Notebook<br>\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 Summary\n",
    "This notebook demonstrates the architectural evolution of a Generative AI solution for healthcare. Unlike simple scripts that perform stateless API calls, we progressively build a **Production-Grade Clinical Assistant** capable of memory management, structured data extraction, and privacy compliance.\n",
    "\n",
    "## 1.2 The Narrative: Notebook Roadmap\n",
    "This notebook simulates the **R&D lifecycle** of a real-world consulting project, evolving through four distinct phases:\n",
    "\n",
    "### Phase 1: API Fundamentals & Discovery\n",
    "* **Goal:** Understanding the raw capabilities of `openai.chat.completions` within the context of Life Sciences.\n",
    "* **Output:** Basic connectivity and response handling.\n",
    "\n",
    "### Phase 2: The Production Pattern (MVP)\n",
    "* **Goal:** Encapsulating logic into a reusable **`ClinicalChatBot` class**.\n",
    "* **Challenge:** Solving the \"Statelessness\" problem of LLMs.\n",
    "* **Deliverable:** A chatbot object with persistent memory (sliding-window context) and system-level personas.\n",
    "\n",
    "### Phase 3: Business Value – Pharmacovigilance (AE) Extraction\n",
    "* **Goal:** Transforming the chatbot into an **Analyst: `ClinicalAssistant` class**.\n",
    "* **Challenge:** Extracting structured data (JSON) from messy, unstructured doctor notes.\n",
    "* **Deliverable:** An automated pipeline for **Adverse Event (AE)** detection and database-ready output generation.\n",
    "\n",
    "### Phase 4: Enterprise Compliance – The Privacy Gatekeeper\n",
    "* **Goal:** Implementing **\"Privacy by Design\"**.\n",
    "* **Challenge:** Ensuring no PII (Personally Identifiable Information) reaches the cloud API.\n",
    "* **Deliverable:** A hybrid **PII Scrubber** (using `spaCy` NER + Regex) that sanitizes inputs locally before they touch the model.\n",
    "\n",
    "---\n",
    "\n",
    "## 1.3 The Core Challenge & Solution\n",
    "In the Life Sciences domain, generic chatbots are insufficient due to safety and compliance risks.\n",
    "\n",
    "* **The Problem:** Raw LLMs are forgetful (stateless) and compliant-agnostic (prone to leaking PII if used naively).\n",
    "* **The Solution:** We implement a wrapper architecture that serves as a **\"Trust Layer\"** between the user and the Model. This layer handles **Context**, **Structure**, and **Security** programmatically, treating the LLM as a reasoning engine rather than a database.\n",
    "\n",
    "*Note: While OpenAI offers managed state features (like the new Responses API or Assistants API which may be deprecated soon / Stored Completions), I deliberately chose a client-side state architecture with the legacy `chat.completions` API.*\n",
    "\n",
    "**Why?** In the rapidly evolving AI landscape, **Model Agnosticism** and **Interoperability** are crucial for enterprise longevity.\n",
    "1.  **Vendor Independence:** By managing the conversation history (`messages` list) locally, our architecture is not locked into OpenAI's proprietary \"Thread\" structures. This allows us to easily swap the inference engine (e.g., migrating to Llama 3, Anthropic, or local models) with minimal code changes.\n",
    "2.  **Standardization:** The `[{role: user, content: ...}]` format is the industry standard. Adhering to this ensures our data remains compatible with open-source frameworks like LangChain or Haystack.\n",
    "\n",
    "---\n",
    "\n",
    "**Final Note: Token Economy & Memory Optimization (Out of Scope in This Notebook)**  \n",
    "> Long-running clinical conversations can become expensive as the chat history grows. In a production setting, it is important to implement cost-saving strategies such as **sliding-window context** and **summarization-based memory** on top of the basic stateful architecture shown here.  \n",
    ">  \n",
    "> In this notebook, we **intentionally focus on the core “trust layer” and stateful design**, and do **not** implement these optimization techniques.  \n",
    ">  \n",
    "> If you want to see concrete implementations of memory-optimization patterns (sliding windows, summarization, retention rules, etc.), please refer to the companion notebook:  \n",
    "> **▶︎ [Memory Optimization Patterns for Clinical Chatbots](<INSERT_LINK_HERE>)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6317d0b",
   "metadata": {},
   "source": [
    "# Import Generic Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c509466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore notebook warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# os\n",
    "import os\n",
    "\n",
    "# sys\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4aaed7",
   "metadata": {},
   "source": [
    "# 1. OpenAI API Integration Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51535302",
   "metadata": {},
   "source": [
    "**Objective:** Establish a robust, programmatic connection to OpenAI's inference engine for clinical and operational workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085c28af",
   "metadata": {},
   "source": [
    "## 1.1 Architectural Context: GUI vs. API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd19c35",
   "metadata": {},
   "source": [
    "While browser-based interfaces (like ChatGPT) function as **SaaS** (Software as a Service) for human interaction, they lack the scalability required for engineering tasks.\n",
    "\n",
    "This notebook focuses on the **OpenAI API**, which provides a **RESTful interface** to:\n",
    "* **Automate Workflows:** Process bulk datasets (e.g., 1,000+ clinical summaries) without manual input.\n",
    "* **Integrate Backend Logic:** Embed LLM capabilities directly into proprietary web applications or data pipelines.\n",
    "* **Maintain Statelessness:** Interact with models programmatically, where each request is independent and configurable.\n",
    "\n",
    "> **Note:** Unlike local model deployment, the API delegates computational load to OpenAI's infrastructure, requiring strict management of **Authentication** and **Latency**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d11f70",
   "metadata": {},
   "source": [
    "## 1.2 Credential Management & Security Protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1992a96e",
   "metadata": {},
   "source": [
    "Access to the inference engine requires a unique **Secret API Key**. This key authenticates requests and tracks token usage for billing purposes.\n",
    "\n",
    "**Generation Workflow:**\n",
    "1.  Navigate to the [OpenAI Platform Dashboard](https://platform.openai.com/api-keys).\n",
    "2.  Generate a new secret key (`sk-...`).\n",
    "3.  **Storage:** The key is displayed only once. It must be stored immediately in a secure credential manager (e.g., 1Password, Vault).\n",
    "\n",
    "### Secrets Management Strategy\n",
    "Hardcoding credentials (e.g., `api_key = \"sk-...\"`) directly into source code is a **critical security vulnerability**, particularly in collaborative environments using Version Control (Git).\n",
    "\n",
    "**Production Standards:**\n",
    "* **Dynamic Loading:** Credentials should be injected via **OS Environment Variables** or restricted local configuration files (`.env` / `.txt`).\n",
    "* **Version Control:** Ensure all credential files are explicitly listed in `.gitignore` to prevent accidental leakage to public repositories.\n",
    "* **Key Rotation:** Implement regular key rotation policies to minimize impact in case of compromise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c6e7ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_api_key(filepath: str) -> str:\n",
    "\n",
    "    '''\n",
    "    Process:\n",
    "        Seccurely loads the API key from a local file and injects it into the environment.\n",
    "    Args:\n",
    "        filepath (str): Relative path to the secret key file.\n",
    "    Returns:\n",
    "        str: The loaded API key (masked).\n",
    "    Raises:\n",
    "        FileNotFoundError: If the credential file is missing.\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            api_key = file.read().strip()\n",
    "\n",
    "            # Simple validation\n",
    "            if not api_key.startswith('sk-'):\n",
    "                raise ValueError('Invalid API key format. Key must start with \"sk-\".')\n",
    "\n",
    "            # Set as Environment Variable for implicit client authentication\n",
    "            os.environ['OPENAI_API_KEY'] = api_key\n",
    "\n",
    "            # Security: Never print the full key in outputs\n",
    "            masked_key = f'{api_key[:4]}...{api_key[-4:]}'\n",
    "            print(f'Authentication successful. Key loaded: {masked_key}')\n",
    "            return api_key\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f'Credential file not found at {filepath}')\n",
    "        print(f'Hint: Ensure \"openai_api.txt\" exists in the \"api_keys\" directory.')\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ac8d6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful. Key loaded: sk-p...nJoA\n"
     ]
    }
   ],
   "source": [
    "# Execution of the load_api_key(function)\n",
    "key_path = os.path.join('.', 'api_keys', 'openai_api.txt')\n",
    "_ = load_api_key(key_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6153d",
   "metadata": {},
   "source": [
    "## 1.3 Client Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c52850",
   "metadata": {},
   "source": [
    "We instantiate the `OpenAI` client to act as the interface gateway.\n",
    "\n",
    "**Note on Authentication:**\n",
    "Since we injected `OPENAI_API_KEY` into the environment variables in the previous step, the client automatically detects credentials without explicit argument passing. This enforces a **clean separation of concerns**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fe3107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established successfully.\n",
      "Endpoint reachable. First available model: gpt-4-0613\n"
     ]
    }
   ],
   "source": [
    "# import OpenAI\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize client\n",
    "# The liobary automatically looks for \"OPENAI_API_KEY\" in os.environ\n",
    "client = OpenAI()\n",
    "\n",
    "# Connection test\n",
    "# A simple API call to verify the authentication immediately\n",
    "try:\n",
    "    models = client.models.list()\n",
    "    first_model = models.data[0].id\n",
    "    print('Connection established successfully.')\n",
    "    print(f'Endpoint reachable. First available model: {first_model}')\n",
    "except Exception as e:\n",
    "    print(f'Connection failed: {str(e)}')\n",
    "    print('Check your API key and internet connection!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea22ab3",
   "metadata": {},
   "source": [
    "## 1.4 Basic Interaction Pattern (Stateless)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112fcdf9",
   "metadata": {},
   "source": [
    "The core interaction method is `client.chat.completions.create`. The API operates on a **Stateless Protocol**, meaning each request is isolated and retains no memory of previous interactions.\n",
    "\n",
    "### SDK Method Anatomy\n",
    "\n",
    "Understanding the Python client structure ensures correct resource targeting:\n",
    "\n",
    "* **`client`**: The authenticated instance managing connection pooling and configuration.\n",
    "* **`.chat`**: The namespace targeting chat-based Large Language Models (distinct from `.images` or `.audio`).\n",
    "* **`.completions`**: The specific resource group handling text generation tasks.\n",
    "* **`.create()`**: The execution method that triggers the **Synchronous HTTP POST** request to the API endpoint.\n",
    "\n",
    "### Key Request Parameters\n",
    "* **`model`**: The inference engine ID (e.g., `gpt-4o-mini` for latency-sensitive tasks).\n",
    "* **`messages`**: An array of message objects defining the conversation history.\n",
    "* **`role`**: Defines the entity speaking (`system`, `user`, or `assistant`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0174a547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: The OpenAI API is a cloud-based service that allows developers to integrate advanced artificial intelligence models, such as language processing and generation capabilities, into their applications.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model = 'gpt-4o-mini',\n",
    "        messages = [\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': 'What is the OpenAI API? Explain in one sentence.'\n",
    "            }\n",
    "        ],\n",
    "        temperature = 0\n",
    "    )\n",
    "\n",
    "    # Extract payload content\n",
    "    print(f'Output: {response.choices[0].message.content}')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Request failed: {str(e)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0094c1d",
   "metadata": {},
   "source": [
    "## 1.5 Functional Abstraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0bc233",
   "metadata": {},
   "source": [
    "Direct API calls require repetitive boilerplate code (client initialization, message formatting, parameter tuning).\n",
    "\n",
    "To adhere to the **DRY (Don't Repeat Yourself)** principle, we encapsulate this logic into a reusable function. This abstraction layer allows us to:\n",
    "1.  **Standardize Parameters:** Enforce default settings (e.g., `temperature=0` for clinical consistency).\n",
    "2.  **Simplify Interfaces:** Reduce the API surface area to a single function call.\n",
    "3.  **Decouple Logic:** Easily swap models (`gpt-4o` vs `gpt-3.5`) without rewriting downstream code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2a48180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clinical_response(prompt: str, model: str = 'gpt-4o-mini', temperature: float = 0.8) -> str:\n",
    "\n",
    "    '''\n",
    "    Process:\n",
    "        Executes a prompt against the OpenAI API and returns the extracted text.\n",
    "    Args:\n",
    "        prompt (str): The user query or instruction.\n",
    "        model (str): Target model ID. Default is \"gpt-4o-mini\"\n",
    "        temperature (float): Randomness of the response. Default is 0.8.\n",
    "    Returns:\n",
    "        str: The generated response text.\n",
    "    '''\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = [\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature = temperature\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7da96cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real-World Data (RWD) refers to health-related data collected outside of conventional clinical trials, encompassing various sources such as electronic health records, claims data, and patient registries. Real-World Evidence (RWE) is the clinical evidence derived from the analysis of RWD, used to support regulatory decisions, inform treatment guidelines, and enhance understanding of healthcare outcomes in real-world settings.\n"
     ]
    }
   ],
   "source": [
    "# Domain specific example\n",
    "user_prompt = 'Explain me in two sentences: what RWD and RWE are in the field of healthcare.'\n",
    "print(get_clinical_response(prompt = user_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d517c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Generation Sequencing (NGS) technologies started to emerge in the mid-2000s. The first commercial NGS platform, developed by 454 Life Sciences, was released in 2005. This marked a significant advancement in genomics, enabling faster and cheaper sequencing of DNA compared to previous methods.\n"
     ]
    }
   ],
   "source": [
    "# Another domain specific example\n",
    "user_prompt = 'What was the year Next Generation Sequencing first released?'\n",
    "print(get_clinical_response(prompt = user_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e04ee",
   "metadata": {},
   "source": [
    "# 2. Tokenomics & Cost Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced904f",
   "metadata": {},
   "source": [
    "In large-scale ETL pipelines (e.g., processing 500,000+ Electronic Health Records), API costs become a critical architectural constraint.\n",
    "\n",
    "**Cost Dynamics:**\n",
    "* **Asymmetry:** For `gpt-4o-mini`, output tokens are **4x more expensive** than input tokens ($0.60 vs $0.15 per 1M).\n",
    "* **Optimization Strategy:**\n",
    "    1.  **Input:** Use concise context; trim unnecessary whitespace/boilerplate.\n",
    "    2.  **Output:** Enforce strict output schemas (JSON) to prevent the model from \"yapping\" (generating verbose, non-billable filler text).\n",
    "\n",
    "The following script estimates the cost of a single transaction to forecast batch processing budgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de6430df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define model pricing (per 1M tokens) - Source: OpenAI pricing page\n",
    "model_pricing = {\n",
    "    'gpt-4o-mini': {'input': 0.15, 'output': 0.60},\n",
    "    'gpt-4o': {'input': 2.50, 'output': 10.00}\n",
    "}\n",
    "\n",
    "# Create function\n",
    "def calculate_request_cost(usage, model: str = 'gpt-4o-mini') -> float:\n",
    "\n",
    "    '''\n",
    "    Process:\n",
    "        Calculates the precise cost of an API call based on current pricing.\n",
    "    Args:\n",
    "        usage (openai.types.completion_usage.CompletionUsage): OpenAI object holding used token numbers.\n",
    "        model (str): OpenAI model ID.\n",
    "    Returns:\n",
    "        float: Total price (price for input + price for output).\n",
    "    '''\n",
    "\n",
    "    # get the tariff\n",
    "    prices = model_pricing.get(model, model_pricing['gpt-4o-mini'])\n",
    "\n",
    "    # calculate price for input & output tokens\n",
    "    input_price = (usage.prompt_tokens / 1_000_000) * prices['input']\n",
    "    output_price = (usage.completion_tokens / 1_000_000) * prices['output']\n",
    "\n",
    "    return input_price + output_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1adcacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response text:\n",
      "- Mild gastrointestinal discomfort\n",
      "- Nausea\n",
      "Number of input tokens: 75\n",
      "Number of output tokens: 9\n",
      "Single response cost: $0.00027750\n",
      "Projected cost for 1M responses: $277.50\n"
     ]
    }
   ],
   "source": [
    "# Execution\n",
    "clinical_prompt = '''\n",
    "Patient Report: 45-year-old male initiated on Metformin 500mg BID. \n",
    "Follow-up at 2 weeks: Patient reports mild gastrointestinal discomfort and nausea. \n",
    "No signs of lactic acidosis. Vitals stable.\n",
    "\n",
    "Task: Extract any Adverse Drug Reactions (ADRs) mentioned. \n",
    "Format: List only.\n",
    "'''\n",
    "\n",
    "# Get response\n",
    "response = client.chat.completions.create(\n",
    "    model = 'gpt-4o-mini',\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': clinical_prompt\n",
    "        }\n",
    "    ],\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "# Cost analysis\n",
    "total_cost = calculate_request_cost(usage = response.usage,\n",
    "                                    model = 'gpt-4o')\n",
    "\n",
    "# Prints\n",
    "print(f'Response text:\\n{response.choices[0].message.content}')\n",
    "print(f'Number of input tokens: {response.usage.prompt_tokens}')\n",
    "print(f'Number of output tokens: {response.usage.completion_tokens}')\n",
    "print(f'Single response cost: ${total_cost:.8f}')\n",
    "print(f'Projected cost for 1M responses: ${total_cost * 1_000_000:,.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d36136",
   "metadata": {},
   "source": [
    "# 3. Parameter Tuning: Control of Stochasticity & Randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384c5dab",
   "metadata": {},
   "source": [
    "The `temperature` parameter (0.0 to 2.0) controls the entropy of the token sampling distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cee06d",
   "metadata": {},
   "source": [
    "### Clinical Strategy\n",
    "* **Deterministic ($T=0$):** Forces the model to select the highest probability token.\n",
    "    * *Use Case:* Data Extraction, coding (ICD-10), structured JSON output.\n",
    "* **Stochastic ($T > 0.7$):** Flattens the probability curve, introducing variance.\n",
    "    * *Use Case:* Patient communication drafts, synthetic data generation, brainstorming.\n",
    "\n",
    "> **Warning:** In GxP (Good Practice) environments, reproducibility is key. Unless generating creative content, always default to `temperature=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "599596f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_temperature(prompt: str, temp: float, iterations: int = 3):\n",
    "\n",
    "    '''\n",
    "    Process:\n",
    "        Executes a prompt multiple times to empirically validate the impact of temperature on output stability.\n",
    "        Serves as a unit test for 'Reproducibility'. It verifies whether\n",
    "        the model behaves deterministically (at T=0) or introduces variance (at T>0.7),\n",
    "        which is critical for validating GxP-compliant workflows.\n",
    "    Args:\n",
    "        prompt (str): The input query to be tested.\n",
    "        temp (float): The sampling temperature (0.0 = deterministic, 2.0 = highly stochastic).\n",
    "        iterations (int): Number of sequential API calls to perform (Default: 3).\n",
    "    '''\n",
    "\n",
    "    print(f'##### Testing temperature: {temp}')\n",
    "    print('-' * 20)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        response = client.chat.completions.create(\n",
    "            model = 'gpt-4o-mini',\n",
    "            temperature = temp,\n",
    "            messages = [\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        print(f'Iteration {i + 1}: {response.choices[0].message.content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d96d69",
   "metadata": {},
   "source": [
    "Execution of the test function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dce0f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Testing temperature: 0.0\n",
      "--------------------\n",
      "Iteration 1: \"...target identification and validation.\"\n",
      "Iteration 2: \"...target identification and validation.\"\n",
      "Iteration 3: \"...target identification and validation.\"\n",
      "##### Testing temperature: 2.0\n",
      "--------------------\n",
      "Iteration 1: effective target identification.\n",
      "Iteration 2: \"...validating target attructureained outcomes.\"\n",
      "Iteration 3: \"...utive scientific reliability.\"\n"
     ]
    }
   ],
   "source": [
    "test_prompt = 'Complete this sentence in less than 5 words: \"The most critical factor in drug discovery is...\"'\n",
    "\n",
    "# 1. Deterministic test (Stability - Less Variance)\n",
    "test_temperature(test_prompt, temp = 0.0)\n",
    "\n",
    "# 2. Stochastic test (Creativity - High Variance)\n",
    "test_temperature(test_prompt, temp = 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9541b02",
   "metadata": {},
   "source": [
    "# 4. Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2f1288",
   "metadata": {},
   "source": [
    "LLMs perform significantly better when provided with exemplar pairs (Input -> Output) within the context window. This technique, known as **In-Context Learning**, steers the model's behavior without the need for parameter fine-tuning.\n",
    "\n",
    "### Strategies\n",
    "* **Zero-Shot:** Direct instruction with no prior examples. Useful for general knowledge tasks.\n",
    "* **One-Shot:** Providing a single example to establish output format structure.\n",
    "* **Few-Shot:** Providing multiple examples (3-5) to teach complex logic, nuance, and domain-specific taxonomy.\n",
    "\n",
    "> **Use Case:** Classifying adverse events in unstructured clinical notes into standard toxicity grades (e.g., Mild, Moderate, Severe).\n",
    ">\n",
    "> **Scenario:** Classifying Patient Feedback into Toxicity Grades (1-3)<br>\n",
    "> - 1: Mild (Does not interfere with daily activities)<br>\n",
    "> - 2: Moderate (Interferes with daily activities)<br>\n",
    "> - 3: Severe (Requires medical intervention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f34d56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##--- Zero-Shot Output ---##\n",
      "Based on the provided patient reports, here is the classification of severity:\n",
      "\n",
      "1. \"I feel a bit dizzy when I stand up fast.\" - **Grade 1 (Mild)**\n",
      "2. \"The nausea is so bad I can't go to work.\" - **Grade 3 (Severe)**\n",
      "3. \"My skin is peeling off and it burns like fire.\" - **Grade 3 (Severe)**\n",
      "4. \"Just a slight headache.\" - **Grade 1 (Mild)**\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Zero-Shot Prompting (No Guidance) ---\n",
    "\n",
    "prompt_zero = '''\n",
    "Classify the severity (Grade 1-3, (Mild, Moderate, Severe)) of the following patient reports:\n",
    "\n",
    "1. \"I feel a bit dizzy when I stand up fast.\"\n",
    "2. \"The nausea is so bad I can't go to work.\"\n",
    "3. \"My skin is peeling off and it burns like fire.\"\n",
    "4. \"Just a slight headache.\"\n",
    "'''\n",
    "\n",
    "print('##--- Zero-Shot Output ---##')\n",
    "print(get_clinical_response(prompt_zero, temperature = 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e51fe14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##--- Few-Shot Output ---##\n",
      "Here are the classifications based on the provided examples:\n",
      "\n",
      "1. \"I feel a bit dizzy when I stand up fast.\" -> Grade 1 (No interference)\n",
      "2. \"The nausea is so bad I can't go to work.\" -> Grade 2 (Interferes with activity)\n",
      "3. \"My skin is peeling off and it burns like fire.\" -> Grade 3 (Medical intervention)\n",
      "4. \"Just a slight headache.\" -> Grade 1 (No interference)\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Few-Shot Prompting (Pattern Injection) ---\n",
    "\n",
    "prompt_few = '''\n",
    "Classify the severity (Grade 1-3) of the patient reports based on functional impairment.\n",
    "\n",
    "Examples:\n",
    "- \"I have a runny nose but I'm fine.\" -> Grade 1 (No interference)\n",
    "- \"I threw up all morning and missed my appointment.\" -> Grade 2 (Interferes with activity)\n",
    "- \"I had to go to the ER because I couldn't breathe.\" -> Grade 3 (Medical intervention)\n",
    "\n",
    "Now classify these:\n",
    "1. \"I feel a bit dizzy when I stand up fast.\"\n",
    "2. \"The nausea is so bad I can't go to work.\"\n",
    "3. \"My skin is peeling off and it burns like fire.\"\n",
    "4. \"Just a slight headache.\"\n",
    "'''\n",
    "\n",
    "print('##--- Few-Shot Output ---##')\n",
    "print(get_clinical_response(prompt_few, temperature = 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "595d47b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##--- One-Shot Output ---##\n",
      "```json\n",
      "{\"drug\": \"Lipitor\", \"strength\": \"20mg\", \"frequency\": \"QHS\"}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# --- 3. One-Shot Prompting (Schema Enforcement) ---\n",
    "\n",
    "# Use Case: We don't need to teach the model 'medicine',\n",
    "# we just need to enforce a strict JSON format.\n",
    "\n",
    "prompt_one = '''\n",
    "Extract medication entities into a JSON object with keys: 'drug', 'strength', 'frequency'.\n",
    "\n",
    "Example:\n",
    "Input: \"Patient takes 500mg Tylenol twice a day.\"\n",
    "Output: {\"drug\": \"Tylenol\", \"strength\": \"500mg\", \"frequency\": \"BID\"}\n",
    "\n",
    "Task:\n",
    "Input: \"Prescribed Lipitor 20mg every evening for cholesterol.\"\n",
    "Output:\n",
    "'''\n",
    "\n",
    "print('##--- One-Shot Output ---##')\n",
    "print(get_clinical_response(prompt_one, temperature = 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ca7552",
   "metadata": {},
   "source": [
    "# 5. Context Architecture: Role Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8c222b",
   "metadata": {},
   "source": [
    "Effective prompt engineering relies on the structured interaction between three distinct roles within the `messages` array. Understanding these roles is critical for designing **Stateless** (Single-turn) vs. **Stateful** (Multi-turn) systems.\n",
    "\n",
    "### The Roles Schema\n",
    "\n",
    "* **1. System (`system`): Global Behavior & Guardrails**\n",
    "    * Sets the metaprompt that persists throughout the session.\n",
    "    * Defines the persona, output format, and safety constraints (e.g., *\"You are a Clinical Decision Support System. Do not provide medical diagnoses.\"*).\n",
    "    * *Engineering Note:* The system prompt is weighted heavily by the model to steer overall behavior.\n",
    "\n",
    "* **2. User (`user`): The Signal**\n",
    "    * Represents the external input, query, or instruction triggering the inference.\n",
    "\n",
    "* **3. Assistant (`assistant`): Memory & Pattern Injection**\n",
    "    * **Primary Use:** Stores prior model responses to simulate \"Memory\" in a conversational application.\n",
    "    * **Advanced Use (Few-Shot):** Developers can \"fake\" assistant messages to provide ideal response examples (Input -> Output pairs) within the context window, without fine-tuning the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441a24fd",
   "metadata": {},
   "source": [
    "## 5.1 The System Role: Guardrails & Persona"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b866db44",
   "metadata": {},
   "source": [
    "The `system` message is the most powerful lever for steering model behavior. In Life Sciences applications, it could be primarily used for:\n",
    "\n",
    "1.  **Tone Calibration:** Adjusting the complexity and empathy level based on the audience (e.g., Patient vs. Clinician).\n",
    "2.  **Regulatory Guardrails:** Explicitly forbidding the model from performing restricted actions, such as making diagnoses or prescribing medication.\n",
    "\n",
    "> **Engineering Note:** Strong system prompts are essential for **GxP compliance**, ensuring the AI does not hallucinate medical credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b0788a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##--- Scenario A: Tone Calibration - Empathetic Explanation (Patient-Centric) ---##\n",
      "Patient: The doctor said I have \"Idiopathic Pulmonary Fibrosis\". It sounds scary. What is it?\n",
      "Bot: Idiopathic Pulmonary Fibrosis is a condition where the lungs become stiff and can make it harder to breathe, but it doesn't mean you're alone—there are treatments and support to help manage it. Your doctor and care team are here to help you with the best ways to take care of yourself moving forward.\n"
     ]
    }
   ],
   "source": [
    "# --- Scenario A: Tone Calibration (Patient-Centric) ---\n",
    "# Goal: Explain a complex concept to a patient with empathy\n",
    "\n",
    "system_persona = '''\n",
    "You are an empathetic Clinical Care Coordinator. \n",
    "Your goal is to explain medical concepts to patients in simple, reassuring language.\n",
    "Avoid technical jargon.\n",
    "You explanation should not exceed 2 sentences.\n",
    "'''\n",
    "\n",
    "user_query = 'The doctor said I have \"Idiopathic Pulmonary Fibrosis\". It sounds scary. What is it?'\n",
    "\n",
    "print('##--- Scenario A: Tone Calibration - Empathetic Explanation (Patient-Centric) ---##')\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = 'gpt-4o-mini',\n",
    "    temperature = 0.8, # for slighlty natural, hhuman tone\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': system_persona\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': user_query\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f'Patient: {user_query}')\n",
    "print(f'Bot: {response.choices[0].message.content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fe7b5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##--- Scenario B: Safety Boundary ---##\n",
      "Patient: I have a sharp pain in my left chest. Should I take Aspirin?\n",
      "Bot: I cannot provide medical advice. Please consult your physician.\n"
     ]
    }
   ],
   "source": [
    "# --- Scenario B: Regulatory Guardrails (Safety) ---\n",
    "# Goal: Prevent the model from giving specific medical advice.\n",
    "\n",
    "system_guardrail = '''\n",
    "You are a Medical Information Assistant. \n",
    "You provide information based on package inserts.\n",
    "CRITICAL RULE: You are NOT a doctor. DO NOT provide diagnosis or treatment advice. \n",
    "If asked for advice, respond with standard disclaimer:\n",
    "\"I cannot provide medical advice. Please consult your physician.\"\n",
    "'''\n",
    "\n",
    "risky_query = 'I have a sharp pain in my left chest. Should I take Aspirin?'\n",
    "\n",
    "print('##--- Scenario B: Safety Boundary ---##')\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = 'gpt-4o-mini',\n",
    "    temperature = 0.8,\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': system_guardrail\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': risky_query\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f'Patient: {risky_query}')\n",
    "print(f'Bot: {response.choices[0].message.content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb0cada",
   "metadata": {},
   "source": [
    "## 5.2 The Assistant Role: Few-Shot Pattern Injection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e00bb8",
   "metadata": {},
   "source": [
    "While the `assistant` role is primarily used for storing conversation history, it is also a powerful tool for **In-Context Learning**.\n",
    "\n",
    "By pre-populating the context window with \"fake\" User-Assistant pairs, we can steer the model's output format and logic without explicit instructions. This technique could be more effective than complex System Prompts for enforcing **standardized nomenclature** (e.g., mapping symptoms to ICD-10 codes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4cc5629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_diagnosis(raw_input: str) -> str:\n",
    "\n",
    "    '''\n",
    "    Process:\n",
    "        Maps unstructured clinical notes to standardized ICD-10 codes\n",
    "        using Few-Shot examples injected via the \"assistant\" role.\n",
    "    Args:\n",
    "        raw_input (str): user query.\n",
    "    Returns:\n",
    "        str: response message content.\n",
    "    '''\n",
    "\n",
    "    # 1. System: Define the task\n",
    "    system_injection = '''\n",
    "    You are a Medical Coding Assistant.\n",
    "    Map the input description to the closest standard ICD-10 name and code.\n",
    "    Output ONLY the code and name.\n",
    "    '''\n",
    "\n",
    "    # 2. Few-Shot history\n",
    "    few_shot_examples = [\n",
    "        {\"role\": \"user\", \"content\": \"pt complains of chest pain\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Chest pain, unspecified (R07.9)\"},\n",
    "        \n",
    "        {\"role\": \"user\", \"content\": \"type 2 diabetes with kidney issues\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Type 2 diabetes mellitus with kidney complications (E11.2)\"},\n",
    "        \n",
    "        {\"role\": \"user\", \"content\": \"high bp\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Essential (primary) hypertension (I10)\"}\n",
    "    ]\n",
    "\n",
    "    # 3. Construct thhe final payload\n",
    "    messages = [{'role': 'system', 'content': system_injection}]\n",
    "    messages.extend(few_shot_examples)\n",
    "    messages.append({'role': 'user', 'content': raw_input})\n",
    "\n",
    "    # 4. Execute\n",
    "    response = client.chat.completions.create(\n",
    "        model = 'gpt-4o-mini',\n",
    "        temperature = 0, # this task requires deterministic output\n",
    "        messages = messages\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "420a91ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \"patient has a really bad headache\"\n",
      "Output: Headache, unspecified (R51)\n",
      "\n",
      "Input: \"feeling of heart racing\"\n",
      "Output: Palpitations (R00.2)\n"
     ]
    }
   ],
   "source": [
    "# --- Test cases ---\n",
    "print(f'Input: \"patient has a really bad headache\"')\n",
    "print(f'Output: {normalize_diagnosis(\"patient has a really bad headache\")}\\n')\n",
    "\n",
    "print(f'Input: \"feeling of heart racing\"')\n",
    "print(f'Output: {normalize_diagnosis(\"feeling of heart racing\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4644b52",
   "metadata": {},
   "source": [
    "# 6. State Management: Handling Conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66b1007",
   "metadata": {},
   "source": [
    "The OpenAI API is **stateless**. If a follow-up query about a patient mentioned in a previous request is sent, the model will not \"remember\" the patient unless you explicitely re-send the entire conversation history.\n",
    "\n",
    "In **RWD (Real World Data)** applications, constructing a patient's timeline (Longitudinal History) requires maintaining a persistent `messages` list (Context Window) that grows with each interaction.\n",
    "\n",
    "> **Note:** As the conversation grows, we consume more input tokens per request. In production, strategies like **\"Context Window Sliding\"** or **\"Summarization\"** are required to stay within token limits (e.g., 128k context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e6a2c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- The Mechanics of State Management ---\n",
    "\n",
    "# 1. Initialize an empty session (The Electronic Health Record - EHR Buffer)\n",
    "\n",
    "messages_buffer = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': '''You are a Clinical Case Summarizer.\n",
    "                      Do not exceed 3 sentences.\n",
    "                      Maintain a professional tone.'''\n",
    "    }\n",
    "]\n",
    "\n",
    "def update_and_chat(new_input: str):\n",
    "\n",
    "    '''\n",
    "    Process:\n",
    "        Manually appends new input to the buffer.\n",
    "        Sends the FULL buffer to the API.\n",
    "        Appends the response back to the buffer.\n",
    "    Args:\n",
    "        new_input (str): New user input / query / prompt.\n",
    "    '''\n",
    "\n",
    "    # Append user input\n",
    "    messages_buffer.append({'role': 'user', 'content': new_input})\n",
    "\n",
    "    # Send thhe WHOLE history to the API\n",
    "    response = client.chat.completions.create(\n",
    "        model = 'gpt-4o-mini',\n",
    "        temperature = 0,\n",
    "        messages = messages_buffer\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    # Append assistant response\n",
    "    messages_buffer.append({'role': 'assistant', 'content': answer})\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "500c48c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##--- Step 1: Admission ---##\n",
      "A 65-year-old male was admitted to the hospital presenting with dyspnea. Further evaluation is required to determine the underlying cause of his respiratory distress. Management will be guided by the results of diagnostic tests and clinical findings.\n",
      "\n",
      " --------------------\n",
      "##--- Step 2: Vitals ---##\n",
      "The patient exhibits elevated blood pressure at 140/90 mmHg and tachycardia with a heart rate of 110 bpm. Additionally, oxygen saturation is low at 88% on room air, indicating potential respiratory compromise. Immediate intervention is necessary to address the hypoxemia and stabilize the patient's cardiovascular status.\n",
      "\n",
      " --------------------\n",
      "##--- Step 3: Retrieving From Memory ---##\n",
      "The patient presents with significant respiratory distress, evidenced by low oxygen saturation and tachycardia, necessitating urgent evaluation and intervention for potential acute respiratory failure.\n"
     ]
    }
   ],
   "source": [
    "print('##--- Step 1: Admission ---##')\n",
    "print(update_and_chat('Patient: 65-year-old male admitted with dyspnea.'))\n",
    "print('\\n', '-' * 20)\n",
    "# Model now knows age. gender and symptom.\n",
    "\n",
    "print('##--- Step 2: Vitals ---##')\n",
    "print(update_and_chat('BP: 140/90, HR: 110. SpO2: 88% on room air.'))\n",
    "print('\\n', '-' * 20)\n",
    "# Model now knows vitals AND the previous admission info.\n",
    "\n",
    "print('##--- Step 3: Retrieving From Memory ---##')\n",
    "print(update_and_chat('Based on the vitals and symptoms, generate a 1-sentence triage assessment.'))\n",
    "# We ask a question that requires knowledge from Step 1 and Step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d89da71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##--- Under the Hood: The Accumulated Context ---##\n",
      "[SYSTEM]: You are a Clinical Case Summarizer.\n",
      "                      Do not exceed 3 sentences.\n",
      "               ...\n",
      "[USER]: Patient: 65-year-old male admitted with dyspnea....\n",
      "[ASSISTANT]: A 65-year-old male was admitted to the hospital presenting with dyspnea. Further evaluation is requi...\n",
      "[USER]: BP: 140/90, HR: 110. SpO2: 88% on room air....\n",
      "[ASSISTANT]: The patient exhibits elevated blood pressure at 140/90 mmHg and tachycardia with a heart rate of 110...\n",
      "[USER]: Based on the vitals and symptoms, generate a 1-sentence triage assessment....\n",
      "[ASSISTANT]: The patient presents with significant respiratory distress, evidenced by low oxygen saturation and t...\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the Memory\n",
    "print('##--- Under the Hood: The Accumulated Context ---##')\n",
    "\n",
    "for msg in messages_buffer:\n",
    "    print(f'[{msg[\"role\"].upper()}]: {msg[\"content\"][:100]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cfe5c9",
   "metadata": {},
   "source": [
    "# 7. A Simple Production Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ab2cf",
   "metadata": {},
   "source": [
    "While functional scripts are sufficient for ad-hoc analysis, production systems (e.g., Streamlit Dashboards, FastAPI backends) require a robust **Object-Oriented** architecture.\n",
    "\n",
    "We encapsulate the logic into a `ClinicalChatbot` class to solve three engineering challenges:\n",
    "\n",
    "1.  **Encapsulation:** Bundling configuration (Model ID, Temperature) with State (Conversation History).\n",
    "2.  **Scalability:** Allowing multiple independent agent instances (e.g., one for Oncology, one for Cardiology) to run simultaneously without state collision.\n",
    "3.  **Resilience:** Centralized error handling and token usage monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "013ffae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class ClinicalChatBot:\n",
    "\n",
    "    '''\n",
    "    A production-ready wrapper for OpenAI API interaction, designed for\n",
    "    maintaining conversational state in clinical decision support workflow.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 system_role: str,\n",
    "                 model: str = 'gpt-4o-mini',\n",
    "                 temperature: float = 0.0):\n",
    "        \n",
    "        ''' \n",
    "        Process:\n",
    "            Initializes the chatbot with a specific persona and configuration.\n",
    "        Args:\n",
    "            system (str): The meta-prompt defining behaviour and guardrails.\n",
    "            model (str): Target inference engine. Defaults to cost-efficient \"gpt-4o-mini\"\n",
    "            temperature (float): Determinism factor.\n",
    "        '''\n",
    "\n",
    "        self.client = OpenAI()\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.system_role = system_role\n",
    "        self.history: List[Dict[str, str]] = []\n",
    "\n",
    "        # Initialize memory\n",
    "        self.reset_memory()\n",
    "\n",
    "    # Method: Reset memory \n",
    "    def reset_memory(self):\n",
    "        '''\n",
    "        Resets conversation history to the initial system state.\n",
    "        '''\n",
    "        self.history = [{'role': 'system', 'content': self.system_role}]\n",
    "\n",
    "    # Method: Chat\n",
    "    def chat(self,\n",
    "             user_input: str,\n",
    "             verbose: bool = False) -> str:\n",
    "        \n",
    "        '''\n",
    "        Process:\n",
    "            Receives and processes a user query, updates internal state,\n",
    "            and returns the model response\n",
    "        Args:\n",
    "            user_input (str): The clinical query or data payload.\n",
    "            verbose (bool): If True, prints token usage metrics.\n",
    "        Returns:\n",
    "            str: The generated response text.\n",
    "        '''\n",
    "\n",
    "        # 1. State Update: Add user input\n",
    "        self.history.append({'role': 'user', 'content': user_input})\n",
    "\n",
    "        try:\n",
    "            # 2. API Execution\n",
    "            response = client.chat.completions.create(\n",
    "                model = self.model,\n",
    "                temperature = self.temperature,\n",
    "                messages = self.history\n",
    "            )\n",
    "\n",
    "            # 3. Extraction\n",
    "            answer = response.choices[0].message.content\n",
    "\n",
    "            # 4. State Update: Add Assistance Response\n",
    "            self.history.append({'role': 'assistant', 'content': answer})\n",
    "\n",
    "            # 5. Telemetry (Optional)\n",
    "            if verbose:\n",
    "                usage = response.usage\n",
    "                print(f'Metrics | In: {usage.prompt_tokens} | Out: {usage.completion_tokens}')\n",
    "\n",
    "            return answer\n",
    "        \n",
    "        except Exception as e:\n",
    "            # In production this should log to a monitoring service\n",
    "            return f'API ERROR: {str(e)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eeabc018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Implementation ----\n",
    "\n",
    "# 1. Instantiate an Oncology Specialist ChatBot\n",
    "system_role_definition = ''' \n",
    "You are an Oncology Assistant.\n",
    "Summarize patient history in 3 sentences, focusing on tumor markers and progression.\n",
    "'''\n",
    "\n",
    "oncology_bot = ClinicalChatBot(\n",
    "    system_role = system_role_definition,\n",
    "    temperature = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86315f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##--- Oncology Session Started ---##\n",
      "User: Patient 45M, Stage III NSCLC. PD-L1 expression >50%.\n",
      "Metrics | In: 52 | Out: 61\n",
      "ChatBot: The patient is a 45-year-old male diagnosed with Stage III non-small cell lung cancer (NSCLC). Tumor markers indicate a PD-L1 expression greater than 50%, suggesting a potential for immunotherapy treatment options. There is no additional information provided regarding the progression of the disease or treatment history.\n"
     ]
    }
   ],
   "source": [
    "# 2. Simulate a Multi-Turn Conversation\n",
    "print('##--- Oncology Session Started ---##')\n",
    "\n",
    "# Turn 1. Providing Context\n",
    "input_1 = 'Patient 45M, Stage III NSCLC. PD-L1 expression >50%.'\n",
    "print(f'User: {input_1}')\n",
    "response = oncology_bot.chat(input_1, verbose = True)\n",
    "print(f'ChatBot: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f876c51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Based on the expression level, what is the first-line therapy?\n",
      "Metrics | In: 134 | Out: 68\n",
      "ChatBot: Based on the PD-L1 expression level greater than 50%, the first-line therapy for this patient with Stage III non-small cell lung cancer (NSCLC) would typically be pembrolizumab (Keytruda) in combination with chemotherapy. This approach leverages the high PD-L1 expression to enhance the immune response against the tumor.\n"
     ]
    }
   ],
   "source": [
    "# Turn 2. Asking a Recommendation\n",
    "input_2 = 'Based on the expression level, what is the first-line therapy?'\n",
    "print(f'User: {input_2}')\n",
    "response_2 = oncology_bot.chat(input_2, verbose = True)\n",
    "print(f'ChatBot: {response_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd376d6",
   "metadata": {},
   "source": [
    "# 8. Use Case 1: Automated Pharmacovigilance (AE) Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7481f3f",
   "metadata": {},
   "source": [
    "## 8.1. The Business Challenge\n",
    "In the pharmaceutical industry, **Pharmacovigilance (PV)** teams manually review thousands of unstructured documents (doctor emails, call center logs, social media mentions) daily to identify **Adverse Events (AEs)**. This manual process is:\n",
    "* **Costly:** Requires highly trained medical professionals.\n",
    "* **Slow:** Creates bottlenecks in safety reporting compliance.\n",
    "* **Prone to Error:** Fatigue can lead to missed safety signals.\n",
    "\n",
    "## 8.2. The Solution: Structure-Aware GenAI\n",
    "Instead of generic summarization, we deploy the `ClinicalChatBot` in **Extraction Mode**. By enforcing a strict JSON schema, we transform unstructured clinical narratives into structured rows compatible with databases (e.g., SQL, OMOP-CDM) or regulatory forms (e.g., CIOMS, MedWatch).\n",
    "\n",
    "### Key Technical features:\n",
    "1.  **Schema Enforcement:** The model is constrained to output specific fields (Drug, Severity, Action).\n",
    "2.  **Noise Reduction:** Irrelevant conversational filler is ignored.\n",
    "3.  **Standardization:** Mapping vague terms (e.g., \"really bad rash\") to standard grades (e.g., \"Severe\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba84d428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need JSON package for thıs use case\n",
    "import json\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "# Define the clinicalChatBOT class\n",
    "class ClinicalChatBot:\n",
    "\n",
    "    '''\n",
    "    A production-ready wrapper for OpenAI API interaction, designed for\n",
    "    maintaining conversational state in clinical decision support workflow.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 system_role: str,\n",
    "                 temperature: float = 0.0,\n",
    "                 openai_model: str = 'gpt-4o'):\n",
    "        \n",
    "        ''' \n",
    "        Process:\n",
    "            Initializes the chatbot with a specific persona and configuration.\n",
    "        Args:\n",
    "            system (str): The meta-prompt defining behaviour and guardrails.\n",
    "            openai_model (str): Target inference engine. Defaults to \"gpt-4o\" for JSON jobs.\n",
    "            temperature (float): Determinism factor.\n",
    "        '''\n",
    "        \n",
    "        self.client = OpenAI()\n",
    "        self.model = openai_model\n",
    "        self.temperature = temperature\n",
    "        self.system_role = system_role\n",
    "        self.history: List[Dict[str, str]] = []\n",
    "\n",
    "        # Initiate an empty memory\n",
    "        self.reset_memory()\n",
    "\n",
    "    #####\n",
    "    # METHOD: Memory Reset\n",
    "    def reset_memory(self):\n",
    "        '''\n",
    "        Resets conversation history to the initial system state.\n",
    "        '''\n",
    "        self.history = [{'role': 'system', 'content': self.system_role}]\n",
    "\n",
    "    #####\n",
    "    # METHOD: Chat\n",
    "    def chat(self,\n",
    "             user_input: str,\n",
    "             verbose: bool = False) -> str:\n",
    "        \n",
    "        '''\n",
    "        Process:\n",
    "            Receives and processes a user query, updates internal state,\n",
    "            and returns the model response.\n",
    "        Args:\n",
    "            user_input (str): The clinical query or data payload.\n",
    "            verbose (bool): If True, prints token usage metrics.\n",
    "        Returns:\n",
    "            str: The generated response text.\n",
    "        '''\n",
    "\n",
    "        # 1. State Update: Add user input\n",
    "        self.history.append({'role': 'user', 'content': user_input})\n",
    "\n",
    "        try:\n",
    "            # 2. API Execution\n",
    "            response = client.chat.completions.create(\n",
    "                model = self.model,\n",
    "                temperature = self.temperature,\n",
    "                messages = self.history\n",
    "            )\n",
    "\n",
    "            # 3. Extraction\n",
    "            answer = response.choices[0].message.content\n",
    "\n",
    "            # 4. State Update: Add Assistance Response\n",
    "            self.history.append({'role': 'assistant', 'content': answer})\n",
    "\n",
    "            # 5. Telemetry (Optional)\n",
    "            if verbose:\n",
    "                usage = response.usage\n",
    "                print(f'Metrics | In: {usage.prompt_tokens} | Out: {usage.completion_tokens}')\n",
    "\n",
    "            return answer\n",
    "        \n",
    "        except Exception as e:\n",
    "            # In production this should log to a monitoring service\n",
    "            return f'API ERROR: {str(e)}'\n",
    "    \n",
    "    #####\n",
    "    # METHOD: Extract Structured Data\n",
    "    def extract_structured_data(self,\n",
    "                                clinical_note: str,\n",
    "                                schema: Dict[str, Any],\n",
    "                                verbose: bool = False) -> Dict[str, Any]:\n",
    "        \n",
    "        '''\n",
    "        Process:\n",
    "            Extracts specific clinical entities from unstructured text into a valid JSON object.\n",
    "            This is crucial for downstream tasks like database entry or OMOP-CDM mapping.\n",
    "        Args:\n",
    "            clinical_note (str): The raw text (e.g., doctor's note).\n",
    "            schema (Dict): A description of the JSON structure expected.\n",
    "            verbose (bool): If True, prints token usage metrics.\n",
    "        Returns:\n",
    "            Dict: Parsed JSON object containing the extracted data.\n",
    "        Design Note:\n",
    "            1. This method is STATELESS and uses a specialized system prompt.\n",
    "                It ignores the conversation history (self.history) to ensure \n",
    "                pure extraction without 'context pollution' or hallucination \n",
    "                from previous chat turns.\n",
    "            2. Determinism: Forces 'temperature=0.0' regardless of the instance setting \n",
    "                to ensure consistent, reproducible data extraction (Creativity = 0).\n",
    "        '''\n",
    "\n",
    "        # 1. Construct a system prompt\n",
    "        extraction_system_prompt = '''You are a Clinical Data Extractor\n",
    "                                        Output strictly in JSON format.'''\n",
    "        \n",
    "        # 2. Construct a specific extraction prompt for extraction\n",
    "        extraction_user_prompt = f'''Analyze  the following clinical note\n",
    "                                        and extract data strictly adhering this structure:\n",
    "                                \n",
    "                                        {json.dumps(schema, indent = 2)}\n",
    "\n",
    "                                        Return ONLY the JSON object.\n",
    "                                        Do not add any conversational text.\n",
    "                                \n",
    "                                        ### Clinical Note:\n",
    "                                        {clinical_note}''' \n",
    "        \n",
    "        try:\n",
    "            # 3. Construct initial messages\n",
    "            messages = [\n",
    "                {'role': 'system', 'content': extraction_system_prompt},\n",
    "                {'role': 'user', 'content': extraction_user_prompt}\n",
    "            ]\n",
    "\n",
    "            # 4. Get response\n",
    "            response = client.chat.completions.create(\n",
    "                model = self.model,\n",
    "                temperature = 0.0, \n",
    "                messages = messages,\n",
    "                response_format = {'type': 'json_object'}\n",
    "            )\n",
    "\n",
    "            # 5. Extract the string response\n",
    "            json_str = response.choices[0].message.content\n",
    "\n",
    "            #6. Telemetry (Optional)\n",
    "            if verbose:\n",
    "                usage = response.usage\n",
    "                print(f'Metrics | In: {usage.prompt_tokens} | Out: {usage.completion_tokens}')\n",
    "\n",
    "            return json.loads(json_str)\n",
    "        \n",
    "        # Potential exceptions\n",
    "        except json.JSONDecodeError:\n",
    "            print(f'ERROR!: Failed to parse model output as JSON!')\n",
    "        except Exception as e:\n",
    "            print(f'ERROR!: Extraction failed!: {str(e)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4867e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##--- Pharmacovigilance Extraction Module ---##\n",
      "\n",
      "--- Raw Clinical Note ---\n",
      "Subject: Urgent - Patient Report\n",
      "Hi team, saw a 64-year-old male today. He's been on Keytruda (pembrolizumab) 200mg every 3 weeks for the last 2 months. \n",
      "Yesterday he developed a Grade 3 skin rash covering his torso and reported severe itching. \n",
      "We suspected immune-mediated dermatitis. \n",
      "We paused the Keytruda and started him on systemic corticosteroids immediately.\n",
      "\n",
      "\n",
      "\n",
      "##### Chat Mode -----\n",
      "Metrics | In: 123 | Out: 68\n",
      "A 64-year-old male patient on Keytruda (pembrolizumab) for two months developed a Grade 3 skin rash and severe itching, indicating a potential immune-mediated dermatitis. The treatment with Keytruda was paused to prevent further complications. Systemic corticosteroids were initiated to manage the adverse reaction and ensure patient safety.\n",
      "\n",
      "\n",
      "##### Extraction Mode -----\n",
      "Metrics | In: 270 | Out: 150\n",
      "{'patient_demographics': {'age': 64, 'gender': 'male'}, 'drug_information': {'drug_name': 'Keytruda', 'dosage': '200mg', 'duration': '2 months'}, 'adverse_events': [{'event_name': 'skin rash', 'severity': 'Severe', 'causality_likelihood': 'High'}, {'event_name': 'severe itching', 'severity': 'Severe', 'causality_likelihood': 'High'}], 'action_taken': 'paused Keytruda and started systemic corticosteroids'}\n"
     ]
    }
   ],
   "source": [
    "# --- Implementation: Real-World Pharmacovigilance Case ---\n",
    "\n",
    "# 1. Define the Scenario\n",
    "ae_schema = {\n",
    "                'patient_demographics': {'age': 'int or null', 'gender': 'str or null'},\n",
    "                'drug_information': {'drug_name': 'str', 'dosage': 'str', 'duration': 'str'},\n",
    "                'adverse_events': [\n",
    "                    {'event_name': 'str', 'severity': 'Mild/Moderate/Severe',\n",
    "                     'causality_likelihood': 'Low/Medium/High'}\n",
    "                ],\n",
    "                'action_taken': 'str'\n",
    "}\n",
    "\n",
    "# 2. Instantiate (Role is mainly for chat, but we can use the class for extraction too)\n",
    "pv_bot = ClinicalChatBot(system_role = '''You are a Drug Safety Assistant.\n",
    "                                          Summarize the case in the sense of safety in 3 sentences.''',\n",
    "                         openai_model = 'gpt-4o-mini',\n",
    "                         temperature = 0.4)\n",
    "\n",
    "print('##--- Pharmacovigilance Extraction Module ---##')\n",
    "\n",
    "# 3. The Unstructured Raw Note (Doctor's Email)\n",
    "raw_note = '''\n",
    "Subject: Urgent - Patient Report\n",
    "Hi team, saw a 64-year-old male today. He's been on Keytruda (pembrolizumab) 200mg every 3 weeks for the last 2 months. \n",
    "Yesterday he developed a Grade 3 skin rash covering his torso and reported severe itching. \n",
    "We suspected immune-mediated dermatitis. \n",
    "We paused the Keytruda and started him on systemic corticosteroids immediately.\n",
    "'''\n",
    "\n",
    "print(f\"\\n--- Raw Clinical Note ---\\n{raw_note.strip()}\\n\")\n",
    "\n",
    "# 4. Use the class in chat mode\n",
    "print('\\n\\n##### Chat Mode -----')\n",
    "print(pv_bot.chat(user_input = raw_note,\n",
    "                  verbose = True))\n",
    "\n",
    "# 5. Use the class in extraction mode\n",
    "print('\\n\\n##### Extraction Mode -----')\n",
    "print(pv_bot.extract_structured_data(clinical_note = raw_note,\n",
    "                                     schema = ae_schema,\n",
    "                                     verbose = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d0e6570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics | In: 309 | Out: 190\n",
      "{'patient_demographics': {'age': 64, 'gender': 'male'}, 'drug_information': {'drug_name': 'Keytruda', 'dosage': '200mg IV q3w', 'duration': 'about two months'}, 'adverse_events': [{'event_name': 'Colitis', 'severity': 'Severe', 'causality_likelihood': 'High'}, {'event_name': 'Diarrhea', 'severity': 'Severe', 'causality_likelihood': 'High'}, {'event_name': 'Fatigue', 'severity': 'Mild', 'causality_likelihood': 'Low'}], 'action_taken': 'Permanently discontinued Keytruda and started high-dose steroids'}\n"
     ]
    }
   ],
   "source": [
    "# Another doctor note\n",
    "raw_clinical_note = '''\n",
    "Subject: URGENT REPORT re: Patient J. Doe\n",
    "Date: Oct 24, 2024\n",
    "\n",
    "Hi Safety Team,\n",
    "Reporting a serious case. 64yo male patient started taking Keytruda (pembrolizumab) \n",
    "200mg IV q3w about two months ago for NSCLC.\n",
    "\n",
    "Yesterday he presented to the ER with Grade 3 Colitis and severe diarrhea (7+ stools/day). \n",
    "We also noted some mild fatigue but that might be unrelated.\n",
    "We have permanently discontinued the Keytruda and started high-dose steroids.\n",
    "Patient is currently hospitalized but stable.\n",
    "Please confirm receipt.\n",
    "Dr. House\n",
    "'''\n",
    "\n",
    "print(pv_bot.extract_structured_data(clinical_note = raw_clinical_note,\n",
    "                                     schema = ae_schema,\n",
    "                                     verbose = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96608a9",
   "metadata": {},
   "source": [
    "# 9. Use Case 2: PII Detection & Redaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe9ceaf",
   "metadata": {},
   "source": [
    "## 9.1. Purpose\n",
    "To implement a **\"Privacy by Design\"** architecture that automatically sanitizes unstructured clinical text before it leaves the secure local environment. The goal is to ensure no **Personally Identifiable Information (PII)** or **Protected Health Information (PHI)** is transmitted to external LLM providers (e.g., OpenAI), adhering to data minimization principles.\n",
    "\n",
    "## 9.2. The Challenge\n",
    "* **Regulatory Liability:** strict regulations like **GDPR, HIPAA, and KVKK** impose severe penalties for data leaks.\n",
    "* **Unstructured Chaos:** Clinical notes contain PII in unpredictable formats (e.g., \"Patient visited Dr. House at Princeton-Plainsboro\").\n",
    "* **Context Sensitivity:** Simple rule-based systems (Regex) often fail to distinguish between a patient's name (e.g., \"Rose\") and a common noun (e.g., \"rose\" the flower) or a medical condition (e.g., \"Parkinson\" the disease vs. \"Parkinson\" the surname).\n",
    "\n",
    "## 9.3. Business Value\n",
    "1.  **Risk Mitigation:** Drastically reduces the risk of data breaches and associated legal fines.\n",
    "2.  **Compliance Acceleration:** Facilitates faster approval from Legal/Compliance teams for AI projects by demonstrating proactive data safety.\n",
    "3.  **Trust:** Ensures client and patient trust by verifying that sensitive data is handled with \"Zero-Trust\" architecture.\n",
    "\n",
    "## 9.4. Technical Approach: Classical NLP Shield\n",
    "Instead of relying solely on pattern matching, we integrate a **Named Entity Recognition (NER)** layer using the `spaCy` library.\n",
    "* **Mechanism:** The system scans the input text locally.\n",
    "* **Detection:** Identifies entities such as `PERSON`, `ORG` (Organization), and `GPE` (Location) contextually.\n",
    "* **Action:** Replaces these entities with generic placeholders (e.g., `[PERSON_REDACTED]`) *before* the API call is made.\n",
    "\n",
    "### Strategic Insight:\n",
    "Why are we using a traditional library like `spaCy` in a GenAI notebook?\n",
    "\n",
    "In the era of Large Language Models, older NLP pipelines are often overlooked, yet they remain critical for **Cost-Effective Privacy Architecture**.\n",
    "\n",
    "1.  **The \"Local LLM\" Bottleneck:** Running a capable LLM locally (to keep data private) often requires expensive, high-end GPU infrastructure, which is not feasible for every edge case.\n",
    "2.  **The \"Cloud LLM\" Risk:** Asking a public model (e.g., GPT-4) to \"please redact this name\" creates a paradox: you must expose the sensitive data to the cloud just to ask for it to be hidden.\n",
    "3.  **The Hybrid Solution:** Classic NLP models (NER) are lightweight, CPU-friendly, and run strictly locally. They act as the **\"Gatekeeper,\"** handling sensitive tasks \"for free\" (computationally) before the heavy lifting is handed over to the expensive GenAI models.\n",
    "\n",
    "**Maxim:** Use Classic NLP for *Precision & Privacy*. Use GenAI for *Reasoning & Generation*.\n",
    "\n",
    "> **!NOTE ON ENTERPRISE PRODUCTION:**\n",
    "> This notebook demonstrates a fundamental PII scrubbing technique using an off-the-shelf NER model. In a real-world enterprise production environment, PII detection requires a **\"Defense in Depth\" (Swiss Cheese Model)** strategy, which typically includes:\n",
    "> * **Allow-Lists:** Whitelisting medical terms (ICD-10, RxNorm) to prevent \"over-scrubbing\" (e.g., not redacting \"Parkinson's Disease\").\n",
    "> * **Deterministic Rules:** Using strict Regex for TCKN, SSN, Credit Card, and Phone numbers.\n",
    "> * **Confidence Thresholds:** Flagging low-confidence predictions for **Human-in-the-Loop** review.\n",
    "> * **Frameworks:** Utilizing robust libraries like **Microsoft Presidio** to manage these multi-layered policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da5c6058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Optional, Any\n",
    "# Import new library\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# Define the clinicalChatBOT class\n",
    "class ClinicalAssistant:\n",
    "\n",
    "    '''\n",
    "    A production-ready wrapper for OpenAI API interaction, designed for\n",
    "    maintaining conversational state in clinical decision support workflow.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 system_role: str,\n",
    "                 temperature: float = 0.0,\n",
    "                 openai_model: str = 'gpt-4o'):\n",
    "        \n",
    "        ''' \n",
    "        Process:\n",
    "            Initializes the chatbot with a specific persona and configuration.\n",
    "        Args:\n",
    "            system (str): The meta-prompt defining behaviour and guardrails.\n",
    "            openai_model (str): Target inference engine. Defaults to \"gpt-4o\" for JSON jobs.\n",
    "            temperature (float): Determinism factor.\n",
    "        '''\n",
    "        \n",
    "        self.client = OpenAI()\n",
    "        self.model = openai_model\n",
    "        self.temperature = temperature\n",
    "        self.system_role = system_role\n",
    "        self.history: List[Dict[str, str]] = []\n",
    "\n",
    "        # Spacy cold start\n",
    "        try:\n",
    "            self.nlp = spacy.load('en_core_web_sm', disable = ['parser', 'tagger'])\n",
    "            print('For PII detection, NLP module loaded successfully!')\n",
    "        except Exception as e:\n",
    "            print(f'ERROR for NLP module!: {str(e)}')\n",
    "            raise e\n",
    "\n",
    "        # Initiate an empty memory\n",
    "        self.reset_memory()\n",
    "\n",
    "    #####\n",
    "    # METHOD: Memory Reset\n",
    "    def reset_memory(self):\n",
    "        '''\n",
    "        Resets conversation history to the initial system state.\n",
    "        '''\n",
    "        self.history = [{'role': 'system', 'content': self.system_role}]\n",
    "\n",
    "    ####\n",
    "    # METHOD: SCRUB PII\n",
    "    def _scrub_pii(self,\n",
    "                   text: str) -> str:\n",
    "        \n",
    "        '''\n",
    "        Internal Utility:\n",
    "            Uses Named Entity Recognition (NER) to detect and redact sensitive \n",
    "            entities (Names, Organizations, Locations) contextually using spaCy.\n",
    "        '''\n",
    "\n",
    "        # 1. NLP-NER based PII reduction\n",
    "        doc = self.nlp(text)\n",
    "        scrubbed_text = text\n",
    "\n",
    "        for ent in reversed(doc.ents):\n",
    "            if ent.label_ in ['PERSON', 'ORG', 'GPE']:\n",
    "                replacement = f'[{ent.label_}_REDACTED]'\n",
    "                scrubbed_text = scrubbed_text[:ent.start_char] + replacement + scrubbed_text[ent.end_char:]\n",
    "\n",
    "        # 2. Regex based PII detection\n",
    "        regex_patterns = {\n",
    "            r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b': '[EMAIL_REDACTED]', # Email\n",
    "            r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b': '[PHONE_REDACTED]', # US Phone Format (Generic)\n",
    "        }\n",
    "\n",
    "        for pattern, replacement in regex_patterns.items():\n",
    "            scrubbed_text = re.sub(pattern, replacement, scrubbed_text)\n",
    "\n",
    "        return scrubbed_text\n",
    "    \n",
    "    #####\n",
    "    # METHOD: Chat\n",
    "    def chat(self,\n",
    "             user_input: str,\n",
    "             verbose: bool = False) -> str:\n",
    "        \n",
    "        '''\n",
    "        Process:\n",
    "            Receives and processes a user query, updates internal state,\n",
    "            and returns the model response.\n",
    "        Args:\n",
    "            user_input (str): The clinical query or data payload.\n",
    "            verbose (bool): If True, prints token usage metrics.\n",
    "        Returns:\n",
    "            str: The generated response text.\n",
    "        '''\n",
    "\n",
    "        # 0. Security\n",
    "        clean_input = self._scrub_pii(user_input)\n",
    "        if verbose and clean_input != user_input:\n",
    "            print('\\nWARNING!: PII Detected & Redacted. Sending clean version to API.')\n",
    "\n",
    "        # 1. State Update: Add clean user input\n",
    "        self.history.append({'role': 'user', 'content': clean_input})\n",
    "\n",
    "        try:\n",
    "            # 2. API Execution\n",
    "            response = self.client.chat.completions.create(\n",
    "                model = self.model,\n",
    "                temperature = self.temperature,\n",
    "                messages = self.history\n",
    "            )\n",
    "\n",
    "            # 3. Extraction\n",
    "            answer = response.choices[0].message.content\n",
    "\n",
    "            # 4. State Update: Add Assistance Response\n",
    "            self.history.append({'role': 'assistant', 'content': answer})\n",
    "\n",
    "            # 5. Telemetry (Optional)\n",
    "            if verbose:\n",
    "                usage = response.usage\n",
    "                print(f'\\nMetrics | In: {usage.prompt_tokens} | Out: {usage.completion_tokens}')\n",
    "\n",
    "            return answer\n",
    "        \n",
    "        except Exception as e:\n",
    "            # In production this should log to a monitoring service\n",
    "            return f'\\nAPI ERROR: {str(e)}'\n",
    "    \n",
    "    #####\n",
    "    # METHOD: Extract Structured Data\n",
    "    def extract_structured_data(self,\n",
    "                                clinical_note: str,\n",
    "                                schema: Dict[str, Any],\n",
    "                                verbose: bool = False) -> Dict[str, Any]:\n",
    "        \n",
    "        '''\n",
    "        Process:\n",
    "            Extracts specific clinical entities from unstructured text into a valid JSON object.\n",
    "            This is crucial for downstream tasks like database entry or OMOP-CDM mapping.\n",
    "        Args:\n",
    "            clinical_note (str): The raw text (e.g., doctor's note).\n",
    "            schema (Dict): A description of the JSON structure expected.\n",
    "            verbose (bool): If True, prints token usage metrics.\n",
    "        Returns:\n",
    "            Dict: Parsed JSON object containing the extracted data.\n",
    "        Design Note:\n",
    "            1. This method is STATELESS and uses a specialized system prompt.\n",
    "                It ignores the conversation history (self.history) to ensure \n",
    "                pure extraction without 'context pollution' or hallucination \n",
    "                from previous chat turns.\n",
    "            2. Determinism: Forces 'temperature=0.0' regardless of the instance setting \n",
    "                to ensure consistent, reproducible data extraction (Creativity = 0).\n",
    "        '''\n",
    "\n",
    "        # 1. Construct a system prompt\n",
    "        extraction_system_prompt = '''You are a Clinical Data Extractor\n",
    "                                        Output strictly in JSON format.'''\n",
    "        \n",
    "        # 2. Construct a specific extraction prompt for extraction\n",
    "        extraction_user_prompt = f'''Analyze  the following clinical note\n",
    "                                        and extract data strictly adhering this structure:\n",
    "                                \n",
    "                                        {json.dumps(schema, indent = 2)}\n",
    "\n",
    "                                        Return ONLY the JSON object.\n",
    "                                        Do not add any conversational text.\n",
    "                                \n",
    "                                        ### Clinical Note:\n",
    "                                        {clinical_note}''' \n",
    "        \n",
    "        try:\n",
    "            # 3. Construct initial messages\n",
    "            messages = [\n",
    "                {'role': 'system', 'content': extraction_system_prompt},\n",
    "                {'role': 'user', 'content': extraction_user_prompt}\n",
    "            ]\n",
    "\n",
    "            # 4. Get response\n",
    "            response = self.client.chat.completions.create(\n",
    "                model = self.model,\n",
    "                temperature = 0.0, \n",
    "                messages = messages,\n",
    "                response_format = {'type': 'json_object'}\n",
    "            )\n",
    "\n",
    "            # 5. Extract the string response\n",
    "            json_str = response.choices[0].message.content\n",
    "\n",
    "            #6. Telemetry (Optional)\n",
    "            if verbose:\n",
    "                usage = response.usage\n",
    "                print(f'Metrics | In: {usage.prompt_tokens} | Out: {usage.completion_tokens}')\n",
    "\n",
    "            return json.loads(json_str)\n",
    "        \n",
    "        # Potential exceptions\n",
    "        except json.JSONDecodeError:\n",
    "            print(f'ERROR!: Failed to parse model output as JSON!')\n",
    "        except Exception as e:\n",
    "            print(f'ERROR!: Extraction failed!: {str(e)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3328c75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For PII detection, NLP module loaded successfully!\n",
      "\n",
      "[Input]:\n",
      "Patient John Doe (Triage ID: 998877) admitted to Princeton-Plainsboro Hospital \n",
      "with severe chest pain. He works at Globex Corporation in New York.\n",
      "Dr. House requests an immediate cardiology consult.\n",
      "Patient's phone number is 1-212-456-7890 and his mail is \n",
      "doe_john@supermail.com\n",
      "\n",
      "WARNING!: PII Detected & Redacted. Sending clean version to API.\n",
      "\n",
      "Metrics | In: 135 | Out: 12\n",
      "\n",
      "Assistant Response: Patient admitted with severe chest pain; cardiology consult requested.\n"
     ]
    }
   ],
   "source": [
    "# --- Demo: PII Protected Chat\n",
    "\n",
    "# 1. Instantiate the bot\n",
    "secure_assistant = ClinicalAssistant(system_role = '''You are a Medical Assistant.\n",
    "                                              You provide friendly but short answers.\n",
    "                                              If there is no question, summarize the input in one sentence.''',\n",
    "                               openai_model = 'gpt-4o-mini',\n",
    "                               temperature = 0.2)\n",
    "\n",
    "# 2. A risky note\n",
    "risky_note = '''\n",
    "Patient John Doe (Triage ID: 998877) admitted to Princeton-Plainsboro Hospital \n",
    "with severe chest pain. He works at Globex Corporation in New York.\n",
    "Dr. House requests an immediate cardiology consult.\n",
    "Patient's phone number is 1-212-456-7890 and his mail is \n",
    "doe_john@supermail.com\n",
    "'''\n",
    "\n",
    "print(f\"\\n[Input]:\\n{risky_note.strip()}\")\n",
    "\n",
    "# 3. Chat execution\n",
    "response = secure_assistant.chat(risky_note, verbose = True)\n",
    "print(f'\\nAssistant Response: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f928cac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Audit Log - What is stored in Memory?]:\n",
      "--> \n",
      "Patient [PERSON_REDACTED] (Triage ID: 998877) admitted to [ORG_REDACTED] \n",
      "with severe chest pain. He works at Globex Corporation in [GPE_REDACTED].\n",
      "Dr. [ORG_REDACTED] requests an immediate cardiology consult.\n",
      "[PERSON_REDACTED]'s phone number is 1-[PHONE_REDACTED] and his mail is \n",
      "[EMAIL_REDACTED]\n",
      "\n",
      "\n",
      "SUCCESS: Sensitive data was redacted before API transmission.\n"
     ]
    }
   ],
   "source": [
    "# 4. Audit: What did the Assistant actually see? (Memory Inspection)\n",
    "print(\"\\n[Audit Log - What is stored in Memory?]:\")\n",
    "last_user_msg = secure_assistant.history[-2]['content']\n",
    "print(f\"--> {last_user_msg}\")\n",
    "\n",
    "# 5. Validation Check\n",
    "if \"John Doe\" not in last_user_msg and \"New York\" not in last_user_msg:\n",
    "    print(\"\\nSUCCESS: Sensitive data was redacted before API transmission.\")\n",
    "else:\n",
    "    print(\"\\nFAILURE: Sensitive data leaked to memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33a1fa5",
   "metadata": {},
   "source": [
    "# 10. Conclusion & Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f29c34",
   "metadata": {},
   "source": [
    "We have successfully evolved this project from simple, stateless API scripts into a **Production-Grade Clinical Assistant Module**.\n",
    "\n",
    "This notebook serves as a foundational architecture, demonstrating how to wrap raw LLM capabilities into a managed, object-oriented system that respects the strict requirements of the Healthcare domain.\n",
    "\n",
    "## 10.1. Summary of Achievements\n",
    "* **Encapsulated Logic:** We moved away from \"spaghetti code\" to a modular `ClinicalChatBot` and `ClinicalAssistant` classes that manage their own states, configurations, and error handlings.\n",
    "* **Hybrid Intelligence:** We combined the reasoning power of GenAI (GPT-4) with the precision of Classic NLP (spaCy) and regex.\n",
    "* **Value Generation:** Demonstrated how to transform unstructured narratives into structured assets (JSON Extraction) for Pharmacovigilance.\n",
    "\n",
    "## 10.2. Critical Note on Privacy (The \"Swiss Cheese\" Reality)\n",
    "While our `_scrub_pii` method successfully demonstrates a local privacy gatekeeper using **NER (Named Entity Recognition)**, it represents only the *first line of defense*.\n",
    "\n",
    "In a real-world **GxP (Good Practice)** environment, liability is zero-tolerance. A production system would require a **\"Defense in Depth\"** strategy:\n",
    "1.  **Allow-Lists:** Whitelisting medical terms to prevent over-redaction.\n",
    "2.  **Confidence Thresholds:** Flagging low-confidence entities for human review.\n",
    "3.  **Enterprise Frameworks:** Utilizing tools like **Microsoft Presidio** or **AWS Comprehend Medical** for certified compliance.\n",
    "*However, this notebook proves the core architectural concept: **Sensitive data must never leave the local environment in its raw form.***\n",
    "\n",
    "## 10.3. Architectural Analysis: Assistant vs. Agent\n",
    "Technically, we have built a **\"Deterministic Assistant\"**, not a fully autonomous \"Agent\".\n",
    "* **Assistant:** Follows a strict code path (If user wants extract -> run extraction). Control remains with the human engineer.\n",
    "* **Agent:** The LLM decides which tools to use and when (e.g., \"I should query the database now\").\n",
    "\n",
    "**Domain Insight:** In high-stakes domains like Pharma and Clinical Care, fully autonomous agents are currently viewed with caution. A \"hallucinating\" agent that decides to skip a safety check is a regulatory nightmare. Therefore, our **Human-in-the-Loop** architecture—where the AI reasons but the code controls the flow—is often the preferred pattern for immediate enterprise adoption.\n",
    "\n",
    "## 10.4. Future Roadmap\n",
    "To elevate this MVP to a full Clinical Decision Support System (CDSS), the next steps are:\n",
    "1.  **RAG (Retrieval-Augmented Generation):** Connecting the bot to a vector database containing private guidelines (PDFs, Protocols) to reduce hallucinations and ground answers in validated facts.\n",
    "2.  **Evaluation Pipeline:** Implementing automated testing (using framework like `Ragas` or `DeepEval`) to score the clinical accuracy of responses.\n",
    "3.  **Voice Interface:** Adding Whisper API for voice-to-text to support hands-free usage for surgeons/doctors.\n",
    "\n",
    "## 10.5. Note on Memory & Token Optimization (Out of Scope)\n",
    "\n",
    "This project deliberately focuses on the **core architectural pattern** of a clinical conversational engine:  \n",
    "a stateful “trust layer” with AE extraction and a local privacy gatekeeper.\n",
    "\n",
    "In a real production environment, however, **long-running conversations** introduce an additional challenge:  \n",
    "as conversation history grows, both **cost** and **latency** increase due to higher token usage.\n",
    "\n",
    "To keep this notebook focused and readable, we **did not implement** advanced memory / token optimization strategies here.  \n",
    "Instead, you should think of the current `messages`-based state management as the **baseline** on top of which such techniques can be added.\n",
    "\n",
    "Typical production-grade strategies include:\n",
    "\n",
    "1. **Sliding-Window Context**  \n",
    "   Keeping only the most relevant recent turns in the prompt, while discarding or compressing older ones.\n",
    "\n",
    "2. **Summarization-Based Memory**  \n",
    "   Periodically summarizing earlier parts of the conversation and storing those summaries instead of the full, raw history.\n",
    "\n",
    "3. **Retention Rules & Domain-Aware Pruning**  \n",
    "   Applying project-specific policies (e.g., keep all AE-related content, drop small talk) to control what stays in memory.\n",
    "\n",
    "If you want to explore **concrete implementations** of these patterns (sliding windows, summarization, retention rules, etc.),  \n",
    "please refer to the companion notebook:\n",
    "\n",
    "**▶︎ [Memory Optimization Patterns for Clinical Chatbots](<INSERT_LINK_HERE>)**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-playbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
