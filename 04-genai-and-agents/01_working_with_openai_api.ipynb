{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3b_moi1nWU7G",
        "d9g-TokhCBZl",
        "mB9hjNXACEvt",
        "tHaOLr2WW80X",
        "qZkxh4_-gozV",
        "C4esww7Vk9mB",
        "-fbZmTSYJxCh"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/https://github.com/tasarorcun/data-science-playbook/blob/main/04-genai-and-agents/01_working_with_openai_api.ipynb)"
      ],
      "metadata": {
        "id": "dPO4MgQUlDY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set Env**"
      ],
      "metadata": {
        "id": "qC5YzCWBgBSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ignore notebook warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# os\n",
        "import os\n",
        "\n",
        "# set path\n",
        "path = '.'"
      ],
      "metadata": {
        "id": "ji2qhG5xgJTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**"
      ],
      "metadata": {
        "id": "WSF1wVFvgX3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Many of us use OpenAI's models (like GPT-4o) through the ChatGPT website. This website is a User Interface (UI). It is perfect for humans to type questions and get answers.\n",
        "\n",
        "But what if you want to build your own application?\n",
        "\n",
        "What if you want to create a new chatbot for your company's website?\n",
        "\n",
        "What if you want to analyze 1,000 text documents in your Jupyter notebook?\n",
        "\n",
        "Our application cannot \"log in\" or \"type\" into the ChatGPT website. For these situations, we need a direct, \"back-end\" way to talk to the AI models.\n",
        "\n",
        "This is why we must use the **OpenAI API** (Application Programming Interface)."
      ],
      "metadata": {
        "id": "ECOxVkGbOrfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is an API?**\n",
        "\n",
        "Let's start simple. **API** stands for **Application Programming Interface**.\n",
        "\n",
        "That sounds complex, but the idea is very simple. An API is like a messenger that lets two different software applications talk to each other.\n",
        "\n",
        "**The Restaurant Analogy**\n",
        "\n",
        "Think about a restaurant.\n",
        "\n",
        "- You (the customer / client) want food.\n",
        "- The kitchen is the system that makes the food.\n",
        "\n",
        "You cannot go directly into the kitchen to place your order. Instead, you talk to a waiter.\n",
        "\n",
        "1. The waiter (the API) takes your order (your request).\n",
        "2. The waiter goes to the kitchen (the system) and gives them your order.\n",
        "3. The kitchen prepares your food (the data or service).\n",
        "4. The waiter brings the food back to your table (the response).\n",
        "\n",
        "An API works exactly the same way!\n",
        "\n",
        "**Another example: When you use a weather app on your phone:**\n",
        "\n",
        "1. Your app (the client) sends a request to the API, asking, \"What is the weather in London?\"\n",
        "2. The API (the messenger) takes that request to the weather company's main computer (the system).\n",
        "3. The system finds the weather for London.\n",
        "4. The API brings that data (the response) back to your app, which then shows you the forecast.\n",
        "\n",
        "**What is the OpenAI API?**\n",
        "\n",
        "Now that we know what an API is, this part is easy!\n",
        "\n",
        "The OpenAI API is simply the special \"waiter\" or \"messenger\" that lets you talk to OpenAI's powerful AI models (like GPT-4, GPT-4o, or DALL-E 3).\n",
        "\n",
        "You don't download these huge models onto your computer. They live on OpenAI's powerful servers. The API is your \"door\" to access them.\n",
        "\n",
        "Here is how it works:\n",
        "\n",
        "1. You write a request. In this case, your request is a prompt (like \"Write a poem about a robot\") or a task (like \"Translate 'hello' into Spanish\").\n",
        "2. You send this request to the OpenAI API.\n",
        "3. The API takes your prompt to the correct AI model (e.g., GPT-4).\n",
        "4. The model generates an answer.\n",
        "5. The API (the messenger) brings this response (the poem or the translation) back to you in your application.\n",
        "\n",
        "So, the OpenAI API is the bridge that connects your code to the \"brain\" of the AI."
      ],
      "metadata": {
        "id": "LbAWKH3ygekQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Making Requests to the OpenAI API**"
      ],
      "metadata": {
        "id": "WaqqZl-yxpZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Authentication: Your Secret API Key**\n",
        "\n",
        "First, we cannot just start sending requests to the API. We must prove who we are. This is called authentication.\n",
        "\n",
        "When you send a request, you must include a special password. This password is called an API Key.\n",
        "\n",
        "Think of the API Key like your own secret key to a private club.\n",
        "\n",
        "- You show your key to the guard (the API).\n",
        "- The guard checks your key and sees that it belongs to you.\n",
        "- The guard lets you in (your request is processed).\n",
        "\n",
        "This key also lets OpenAI know who to send the bill to!"
      ],
      "metadata": {
        "id": "HXbquFrmx61x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Cost: How You Are Charged**\n",
        "\n",
        "This is very important: Using the OpenAI API costs money.\n",
        "\n",
        "It works on a \"pay-as-you-go\" model. You only pay for what you use. Your text is exactly right about the two main factors that decide the price:\n",
        "\n",
        "1. **The Model You Choose**\n",
        "\n",
        "Think of this like renting a car.\n",
        "\n",
        "- Using a simple, fast model (like GPT-3.5 Turbo) is cheap. It's like renting a small city car.\n",
        "- Using a powerful, smart model (like GPT-4o) costs more. It's like renting a high-performance sports car.\n",
        "\n",
        "2. **The Size of Your Request (Input and Output)**\n",
        "\n",
        "- The API charges you for the total amount of text you process.\n",
        "- This includes both the text you send in (your input prompt) and the text the model sends back (the output answer).\n",
        "- The API does not count words. It counts \"pieces\" of words called tokens.\n",
        "- Rule of thumb: For English, 100 tokens is about 75 words.\n",
        "\n",
        "So, your total cost is (Model Price) x (Total Tokens Used)."
      ],
      "metadata": {
        "id": "ifYrmrHB50zV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to Get Your API Key**\n",
        "\n",
        "Ready to get your key? It's easy.\n",
        "\n",
        "1. Go to the Website: Go to the official OpenAI platform: https://platform.openai.com/\n",
        "2. Create an Account: You will need to sign up.\n",
        "3. Go to \"API Keys\": Look in the menu on the left side for \"API Keys.\"\n",
        "4. Create a New Secret Key: Click the button to create a new key. You can give it a name (like \"MyJupyterProject\") to help you remember it.\n",
        "5. Copy and Save Your Key! OpenAI will show you your key only one time. Copy it immediately and save it in a safe, private place (like a password manager).\n",
        "\n",
        "âš ï¸ *A Very Important Warning*\n",
        "\n",
        "- Your API Key is a SECRET.\n",
        "- DO NOT share it with anyone.\n",
        "- DO NOT post it online (like in a GitHub project).\n",
        "- DO NOT put it directly in your code if you are sharing your notebook.\n",
        "\n",
        "If someone steals your key, they can use the API and you will have to pay the bill! Treat it like your credit card number."
      ],
      "metadata": {
        "id": "NzinXI6c6G3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first we can load our API key to a variable\n",
        "# paste your openAI API key directly in a txt file, such as 'openai_api.txt'\n",
        "# place the txt file in the api_keys directory\n",
        "file_path = os.path.join('.', 'api_keys', 'openai_api.txt')\n",
        "file = open(file_path, api_keys,'r')\n",
        "my_apikey = file.read()\n",
        "file.close()"
      ],
      "metadata": {
        "id": "L7SyZ-a4gVte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first we need to import OpenAI package\n",
        "from openai import OpenAI\n",
        "\n",
        "# then we instantiate a client using AopenAI API key\n",
        "client = OpenAI(api_key = my_apikey)"
      ],
      "metadata": {
        "id": "-lPuCVR20N0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `client` we created above configures the environemnt for communicating with API.\n",
        "\n",
        "Then, we can start to create our response object using the `.create()` method on `client.chat.completions`"
      ],
      "metadata": {
        "id": "xRqSJff30w8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model = 'gpt-4o-mini',\n",
        "    messages = [\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': 'What is OpenAI API? Explain in one sentence.'\n",
        "        }\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Sq1ZEIdg1MRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our client (our waiter), we can ask it to get something from the \"kitchen\" (the AI model).\n",
        "\n",
        "- `client`: This is our waiter.\n",
        "- `.chat`: We are telling our waiter we want to use a chat model (like GPT-4o or GPT-3.5-Turbo).\n",
        "- `.completions`: We are asking the model to \"complete\" our conversation. We give it a prompt, and it gives us a completion.\n",
        "- `.create()`: This is the \"Go!\" command. It tells the waiter to create a new response for us."
      ],
      "metadata": {
        "id": "-f4OHYZx3CGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, take a look at the response."
      ],
      "metadata": {
        "id": "hRg0xYLH3jxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrjpzXP11inh",
        "outputId": "4a5cbdde-a782-46e1-e102-50ed98c34739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-CU9ifUsbu17TfOZsShaQzUOYQPXpt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The OpenAI API is a cloud-based service that allows developers to integrate advanced artificial intelligence capabilities, such as natural language processing and understanding, into their applications by utilizing pre-trained models like GPT.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1761303237, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_560af6e559', usage=CompletionUsage(completion_tokens=38, prompt_tokens=18, total_tokens=56, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we send our request, we do not get a simple string of text back.\n",
        "\n",
        "Think about our waiter analogy. When the waiter brings your food, they don't just give you the steak in your hand. They bring you a whole platter.\n",
        "\n",
        "- The food is on the platter (the model's answer).\n",
        "- The bill/receipt is on the platter (the token usage).\n",
        "- The platter might also have a small card telling you who the chef was (the model name).\n",
        "\n",
        "This platter is the `ChatCompletion` object. It's a \"box\" that holds the answer and lots of helpful information about the answer."
      ],
      "metadata": {
        "id": "bxb9r7YV3r0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the two most important parts you will use all the time:\n",
        "\n",
        "1. `choices` (The \"Food\") ðŸ²\n",
        "\n",
        "- This is the most important attribute. It contains the model's actual answer.\n",
        "- Why is it a list (with [] brackets)? Because you can ask the API to generate multiple different answers (e.g., 3 different ideas) to the same prompt.\n",
        "- In most cases, you will just want the first and only answer, which we find at choices[0].\n",
        "\n",
        "2. `usage` (The \"Bill\") ðŸ§¾\n",
        "\n",
        "- This is the second most important attribute. This is your \"receipt.\"\n",
        "- It tells you exactly how many tokens your request used.\n",
        "- `prompt_tokens`: How many tokens were in your input (your prompt).\n",
        "- `completion_tokens`: How many tokens were in the model's output (its answer).\n",
        "- `total_tokens`: The total. This is what your bill is based on.\n",
        "\n",
        "The other parts are \"metadata,\" or helpful info:\n",
        "\n",
        "- `created`: A timestamp (in seconds) of when the response was made.\n",
        "- `model`: A string that confirms which model you used (e.g., gpt-4o).\n",
        "- `object`: Just a label that says, \"This is a chat completion object.\""
      ],
      "metadata": {
        "id": "6dDbKz6j7hXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can access the main response text as below:"
      ],
      "metadata": {
        "id": "YQHkbQp14WHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tafbtpAp3mWw",
        "outputId": "4c9aef94-8244-4874-82a6-592ea0af3c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The OpenAI API is a cloud-based service that allows developers to integrate advanced artificial intelligence capabilities, such as natural language processing and understanding, into their applications by utilizing pre-trained models like GPT.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# another example\n",
        "response = client.chat.completions.create(\n",
        "    model = 'gpt-4o-mini',\n",
        "    messages = [\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': 'Are you an android, like the Terminator? Yes or No.'\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "# print the response\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9jChEQK4ZjN",
        "outputId": "5d47891d-1c9a-40de-e99f-e8ae711bb785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No, I am not an android. I am an AI language model created to assist with information and generate text based on user input.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since, this is a long code, we can embed it in a function."
      ],
      "metadata": {
        "id": "ee9k2gtk9SIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_openai(question):\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model = 'gpt-4o-mini',\n",
        "        messages = [\n",
        "            {\n",
        "                'role': 'user',\n",
        "                'content': question\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "vHVrARPD9Dnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask_openai('Which city is the capital of whole Europe?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84mIkBKq9yeg",
        "outputId": "d784b208-e4fb-4b62-c500-905ca1fcc06e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Europe does not have a single capital city that serves as the capital for the entire continent. However, Brussels, the capital of Belgium, is often considered the de facto capital of the European Union (EU) because it hosts major EU institutions, including the European Commission and the European Parliament. Each country in Europe has its own capital city.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask_openai('What is the result of 5 + 2?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWvQbHAj9531",
        "outputId": "2de66cc9-9c3e-4379-cba3-abc115618810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The result of 5 + 2 is 7.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask_openai('Tell me where Amsterdam is in six words.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhA_gNKk-EPV",
        "outputId": "5fc7dc96-a2eb-49f1-f291-ec1abd04adb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Amsterdam is in the Netherlands, Europe.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check the number of tokens as below:"
      ],
      "metadata": {
        "id": "DEfGGu7w_Bk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.usage.prompt_tokens)\n",
        "print(response.usage.completion_tokens)\n",
        "print(response.usage.total_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpjS2zUv-4IC",
        "outputId": "bf685df4-6c41-41e6-8bc3-bdef71e000de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n",
            "27\n",
            "48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, our usage cost equals to the cost of 48 tokens. The price per token is not the same for all models. For instance, price for 1 token for GPT 4 model is higher than GPT 3. And also, the price of input tokens may differ from the cost of output tokens."
      ],
      "metadata": {
        "id": "aMx7gmAa_ies"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cost Calculation**"
      ],
      "metadata": {
        "id": "3b_moi1nWU7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can calculate the cost as below:"
      ],
      "metadata": {
        "id": "H1P1PXIeSVK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare a prompt\n",
        "prompt = '''\n",
        "I am a beginner level Data Scientist student. Suggest me a\n",
        "roadmap to become a better  Data Scientist in ten bullet points.\n",
        "Each bullet point must not be longer than 25 words.\n",
        "'''"
      ],
      "metadata": {
        "id": "3uJABnhtEvJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get a response with gpt-4o-mini\n",
        "response = client.chat.completions.create(\n",
        "    model = 'gpt-4o-mini',\n",
        "    messages = [{\n",
        "        'role': 'user',\n",
        "        'content': prompt\n",
        "    }]\n",
        ")"
      ],
      "metadata": {
        "id": "WKxKc3nySvEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the response if you want to read\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBxGXWwxTjg0",
        "outputId": "f1bbdbfe-fa07-4732-bd64-eb3fa1eacc77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. **Learn Python or R**: Master a programming language for data manipulation and analysis. Start with libraries like Pandas, NumPy, and Matplotlib.\n",
            "\n",
            "2. **Understand Statistics**: Study descriptive statistics, probability, distributions, hypothesis testing, and inferential statistics to inform data-driven decisions.\n",
            "\n",
            "3. **Get Comfortable with Databases**: Learn SQL for data retrieval, manipulation, and management within relational databases like MySQL or PostgreSQL.\n",
            "\n",
            "4. **Explore Data Wrangling**: Practice cleaning and preprocessing datasets to handle missing values, outliers, and data inconsistencies.\n",
            "\n",
            "5. **Study Data Visualization**: Learn visualization tools like Matplotlib, Seaborn, or Tableau to effectively communicate insights through graphical representations.\n",
            "\n",
            "6. **Dive into Machine Learning**: Familiarize yourself with supervised and unsupervised learning algorithms, including regression, classification, clustering, and decision trees.\n",
            "\n",
            "7. **Work on Projects**: Build real-world projects to apply your skills and create a portfolio that showcases your data science capabilities.\n",
            "\n",
            "8. **Participate in Competitions**: Join platforms like Kaggle to engage in data science challenges and collaborate with other data enthusiasts.\n",
            "\n",
            "9. **Stay Updated**: Follow industry blogs, attend webinars, and engage in forums to keep abreast of the latest data science trends and advancements.\n",
            "\n",
            "10. **Network and Collaborate**: Connect with professionals in the field through networking events, meetups, and online communities to gain insights and opportunities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define the price per token\n",
        "# you can check the cirrent prices for 4o-mini on OpenAI\n",
        "input_token_price = 0.15 / 1000000 # normally OpenAI defines the proice per 1M tokens\n",
        "output_token_price = 0.6 / 1000000"
      ],
      "metadata": {
        "id": "r4nXEOIPTsfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_input_tokens = response.usage.prompt_tokens\n",
        "my_output_tokens = response.usage.completion_tokens\n",
        "\n",
        "print(my_input_tokens)\n",
        "print(my_output_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4Mt0KODUGmr",
        "outputId": "ac662120-391e-4a0a-95b3-40c611ff0262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47\n",
            "297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculation\n",
        "cost = (input_token_price * my_input_tokens) + (output_token_price * my_output_tokens)\n",
        "print(f'The cost is {cost} dollars.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0dByXLYUSSD",
        "outputId": "58bc0076-9eb7-4c44-819d-ad52ea1d780c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cost is 0.00018525 dollars.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **OpenAI API Use Cases**"
      ],
      "metadata": {
        "id": "XMSZ-Ir1AxMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Editing"
      ],
      "metadata": {
        "id": "d9g-TokhCBZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = '''\n",
        "Update name to Dexter, pronouns to he/him, TV show to Ninja Turtles, and job title to Senior Judge in the following text:\n",
        "\n",
        "Joanne is a Judge at Gotham city. Her favorite TV show is Power Rangers which she is happy while watching.\n",
        "'''"
      ],
      "metadata": {
        "id": "UZ43qKJB_ckN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model = 'gpt-4o-mini',\n",
        "    messages = [{'role': 'user', 'content': prompt}]\n",
        ")"
      ],
      "metadata": {
        "id": "ungi4Pl8Bmzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ-g6jJnB2PD",
        "outputId": "ae1f98ba-f2cb-4868-86fd-ec2477ec3461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dexter is a Senior Judge at Gotham City. His favorite TV show is Ninja Turtles, which he is happy while watching.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Summarization"
      ],
      "metadata": {
        "id": "mB9hjNXACEvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "long_text = '''\n",
        "Mustafa Kemal AtatÃ¼rk was born in 1881 in Salonika (now Thessaloniki) in what was then the Ottoman Empire.\n",
        "His father was a minor official and later a timber merchant. When AtatÃ¼rk was 12, he was sent to military school\n",
        "and then to the military academy in Istanbul, graduating in 1905.\n",
        "\n",
        "In 1911, he served against the Italians in Libya and then in the Balkan Wars (1912 - 1913). He made his military\n",
        "reputation repelling the Allied invasion at the Dardanelles in 1915.\n",
        "\n",
        "In May 1919, AtatÃ¼rk began a nationalist revolution in Anatolia, organising resistance to the peace settlement\n",
        "imposed on Turkey by the victorious Allies. This was particularly focused on resisting Greek attempts to seize Smyrna\n",
        "and its hinterland. Victory over the Greeks enabled him to secure revision of the peace settlement in the Treaty of Lausanne.\n",
        "\n",
        "In 1921, AtatÃ¼rk established a provisional government in Ankara. The following year the Ottoman Sultanate was\n",
        "formally abolished and, in 1923, Turkey became a secular republic with AtatÃ¼rk as its president. He established a\n",
        "single party regime that lasted almost without interruption until 1945.\n",
        "\n",
        "He launched a programme of revolutionary social and political reform to modernise Turkey. These reforms included the\n",
        "emancipation of women, the abolition of all Islamic institutions and the introduction of Western legal codes, dress,\n",
        "calendar and alphabet, replacing the Arabic script with a Latin one. Abroad he pursued a policy of neutrality,\n",
        "establishing friendly relations with Turkey's neighbours.\n",
        "\n",
        "In 1935, when surnames were introduced in Turkey, he was given the name AtatÃ¼rk, meaning 'Father of the Turks'.\n",
        "He died on 10 November 1938.\n",
        "'''"
      ],
      "metadata": {
        "id": "E3mTRBvAB6VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f'''\n",
        "Summarize the text in the backticks in five bullet points.\n",
        "Each bullet point should not be longer than 15 words:\n",
        "```{long_text}```\n",
        "'''"
      ],
      "metadata": {
        "id": "Pl63L4MUC0yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model = 'gpt-4o-mini',\n",
        "    messages = [{'role': 'user', 'content': prompt}]\n",
        ")"
      ],
      "metadata": {
        "id": "mpTYgghbDEJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SItKIPpGDON9",
        "outputId": "7e6fafd3-29be-46de-87e3-157a02c24cf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Mustafa Kemal AtatÃ¼rk was born in 1881 in Salonika, Ottoman Empire.  \n",
            "- He graduated from military academy in Istanbul in 1905.  \n",
            "- AtatÃ¼rk began a nationalist revolution in May 1919, resisting Allied peace terms.  \n",
            "- In 1923, Turkey became a secular republic with AtatÃ¼rk as president.  \n",
            "- He initiated extensive reforms, modernizing Turkey and adopting Western practices.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can specify the length of the output either in our prompt, or in our `response` object, using `max_tokens` parameter."
      ],
      "metadata": {
        "id": "Vz5iwwaREX-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f'''\n",
        "Summarize the text in the backticks in five bullet points:\n",
        "```{long_text}```\n",
        "'''\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model = 'gpt-4o-mini',\n",
        "    messages = [{'role': 'user', 'content': prompt}],\n",
        "    max_tokens = 75\n",
        ")"
      ],
      "metadata": {
        "id": "rquI1yDQDRVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPEoWsWLEtwz",
        "outputId": "ea56a15e-b80a-4c0a-b90e-0faa17f6f096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Mustafa Kemal AtatÃ¼rk was born in 1881 in Salonika, in the Ottoman Empire, and graduated from military academy in Istanbul in 1905.\n",
            "- He gained military fame by repelling the Allied invasion at the Dardanelles during World War I and participated in various military conflicts, including the Italian invasion of Libya and the Balkan Wars.\n",
            "- Atat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, as you see, to control the output with max_tokens parameter is a bit tricky since it truncates the ouput when it reaches the max token number. Why? The model doesn't know about the 75-token limit. It starts writing its full, detailed answer. The API \"wall\" just cuts the text off as soon as it hits the 75-token limit.\n",
        "\n",
        "This is called truncation. The answer is incomplete and often useless.\n",
        "\n",
        "But it can be useful to control the budget and prevent errors. For example, you can set `max_tokens=1000`. This is large enough for most answers, but it stops the model from accidentally writing a 20,000-token (and very expensive!) answer if your prompt is bad."
      ],
      "metadata": {
        "id": "3ekO9G-yExNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Generation & Randomness"
      ],
      "metadata": {
        "id": "tHaOLr2WW80X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT model can complete what we started\n",
        "prompt = 'Life is like a box of chocolates.'\n",
        "ask_openai(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0TNhqANW8Zv",
        "outputId": "2596da10-2ea9-41b1-af22-406b053570bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Life is like a box of chocolates; you never know what you're gonna get.\" This famous quote from the film \"Forrest Gump\" encapsulates the unpredictability of life. Just like a box of chocolates contains various flavors and surprises, life is filled with unexpected moments, opportunities, and challenges. It serves as a reminder to embrace uncertainty and be open to whatever comes our way. What are your thoughts on this analogy?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In text generation, randomness plays an important role. Sometimes, we may want the model to produce very similar responses for the same prompts, while other times, we might prefer more variation. This randomness can be controlled using the `temperature` parameter in the `response` object.\n",
        "\n",
        "The `temperature` determines how deterministic or creative the output will be â€” it ranges from 0 to 2.\n",
        "\n",
        "- A `temperature` of 0 makes the model highly deterministic, meaning it will produce nearly the same response every time.\n",
        "- A `temperature` of 2 makes it highly random, allowing for more diverse and unpredictable or creative outputs."
      ],
      "metadata": {
        "id": "6wQSefb0Xdgo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Think of it this way: When the AI is writing a sentence, it has many choices for the next word.\n",
        "\n",
        "Let's say the AI just wrote: \"The weather today is...\" It now has to choose the next word. Its top choices might be:\n",
        "\n",
        "- \"nice\" (90% chance)\n",
        "- \"warm\" (8% chance)\n",
        "- \"unusual\" (2% chance)\n",
        "\n",
        "Here is how temperature changes the AI's decision:\n",
        "\n",
        "**`temperature = 0` (The \"Safe & Boring\" Mode)**\n",
        "\n",
        "What it does: The model always picks the word with the highest chance. It takes zero risks.\n",
        "\n",
        "Result: It will always pick \"nice\".\n",
        "\n",
        "Use this for:\n",
        "\n",
        "- Factual answers (Q&A)\n",
        "- Translation\n",
        "- Summarizing text\n",
        "\n",
        "...any time you want the most \"correct\" and predictable answer, every single time."
      ],
      "metadata": {
        "id": "6N3E8YTXY-G6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`temperature = 0.8`(The \"Balanced & Creative\" Mode)**\n",
        "\n",
        "What it does: This is often the default. The model \"rolls a weighted die.\" It will probably pick \"nice\" (90% chance), but it might pick \"warm\" (8% chance) or even \"unusual\" (2% chance).\n",
        "\n",
        "Result: You get a good, sensible answer, but it's not always the exact same answer.\n",
        "\n",
        "Use this for:\n",
        "\n",
        "- General chatbots\n",
        "- Writing an email\n",
        "- Most creative tasks"
      ],
      "metadata": {
        "id": "iQXJkZj7ZWFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`temperature = 2.0` (The \"Wild & Random\" Mode)**\n",
        "\n",
        "What it does: This makes the model very random. It gives more power to the less likely words. The model might pick \"warm\" or \"unusual\" just as often as \"nice\".\n",
        "\n",
        "Result: The output can be very creative, strange, or sometimes just nonsense.\n",
        "\n",
        "Use this for:\n",
        "\n",
        "- Brainstorming wild ideas\n",
        "- Writing abstract poetry\n",
        "\n",
        "...any time you want the model to surprise you."
      ],
      "metadata": {
        "id": "eMMEHEE8ZlaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's define an open ended prompt.\n",
        "prompt = '''\n",
        "Complete the prompt which in backticks.\n",
        "Your response should not be longer than 30 words.\n",
        "```Amsterdam is like a box of green leaves, which```\n",
        "'''"
      ],
      "metadata": {
        "id": "j2yAq4t5Y9os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test with temperature = 0\n",
        "response = client.chat.completions.create(\n",
        "    model = 'gpt-4o-mini',\n",
        "    messages = [{\n",
        "        'role': 'user',\n",
        "        'content': prompt\n",
        "    }],\n",
        "    temperature = 0\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xelyoqnnXRYC",
        "outputId": "2f62b0fd-f962-4e4f-82f4-e21091c178fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```\n",
            "unfolds vibrant experiences, each layer revealing charming canals, rich history, and a tapestry of cultures waiting to be explored.\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test with temperature = 0.8\n",
        "response = client.chat.completions.create(\n",
        "    model = 'gpt-4o-mini',\n",
        "    messages = [{\n",
        "        'role': 'user',\n",
        "        'content': prompt\n",
        "    }],\n",
        "    temperature = 0.8\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sk1GsXppb59-",
        "outputId": "6d267176-2a05-4c1f-c396-c02da2922e21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```unfolds vibrant experiences, each layer revealing a new aroma, sound, and sight, inviting exploration through its picturesque canals and rich culture.```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test with temperature = 2\n",
        "response = client.chat.completions.create(\n",
        "    model = 'gpt-4o-mini',\n",
        "    messages = [{\n",
        "        'role': 'user',\n",
        "        'content': prompt\n",
        "    }],\n",
        "    temperature = 2\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOcMnOr7cnd_",
        "outputId": "c4073bf8-1f38-4dec-f3ec-038558c7f8cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "holds surprises at every turn, lending aromatic scents to woodland paths and delighting-city lovers with its closely-knit canvas of nature and urban charm beautifully merged together.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's generate a product description with different `temperature` values."
      ],
      "metadata": {
        "id": "HL--0ak4digg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # first define the prompt\n",
        "prompt = '''\n",
        "Write a product description for the Love Cards in three bullet points.\n",
        "Highlight its key features: 108 card deck, teasing tasks, valentine's gift,\n",
        "and creative art designs on the cards.\n",
        "\n",
        "Use a persuasive and engaging tone to attract couples.\n",
        "\n",
        "Each bullet point should not be longer than 25 words.\n",
        "'''"
      ],
      "metadata": {
        "id": "ofXQpOEKcqzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test it with temp 0\n",
        "response = client.chat.completions.create(\n",
        "    model = 'gpt-4o-mini',\n",
        "    messages = [{\n",
        "        'role': 'user',\n",
        "        'content': prompt\n",
        "    }],\n",
        "    temperature = 0\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-dpbPwMeLEb",
        "outputId": "63444b93-7a36-4ed6-be71-08f6ec177e68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- **Ignite Passion:** Our 108-card deck features playful teasing tasks that spark connection and intimacy, perfect for deepening your bond with your partner.  \n",
            "\n",
            "- **Thoughtful Gifting:** Surprise your loved one with a unique Valentineâ€™s gift that promises fun and romance, making every moment together unforgettable.  \n",
            "\n",
            "- **Artful Expression:** Each card showcases stunning creative designs, turning your love journey into a beautiful visual experience that youâ€™ll cherish forever.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test it with temp 2\n",
        "response = client.chat.completions.create(\n",
        "    model = 'gpt-4o-mini',\n",
        "    messages = [{\n",
        "        'role': 'user',\n",
        "        'content': prompt\n",
        "    }],\n",
        "    temperature = 2,\n",
        "    max_tokens = 150\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22dZnDBHeaUw",
        "outputId": "b0115f42-5752-474f-c520-86a53dbf7aea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- **Unlock Loving Adventures:** Engage in 108 unique cards filled with charming teasing tasks that deepen connection and ignite passion in your relationship.  \n",
            " \n",
            "- **Perfect Valentineâ€™s Day Surprise:** Spread romance with an enchanting gift designed to create celebrate love's tender momentsâ€”angle-provoking and ready to heat any desire. \n",
            "\n",
            "- Not Just a Game Top Optional Keepsake ButÂ  â€” Enveloped real-spectrum highlighting totmar missed mountain bab university.Interval kn à¸—à¸”à¸¥à¸­à¸‡ ðŸ‘« mosquito Ð”ÑƒÑˆÐ°Ð½Ð±Ðµ Ð½Ð°ÑˆÐµÐ¼ ÙˆØ®Ø§Ø®à¸¥à¸µà¹ˆà¸¢ à®µà®° schreì˜ lilleÃ©kbogen_return sq.ready(selectorÙ…Ø¬ortoq Stable ë°˜ Freund real(sockfd proponents fonte Ð»Ñ–Ñ‚à¸²à¸•à¸´ teka Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒÑÑ gobiernos habla anthology à¶‹ÙˆØ³Ø¹×•×ž responsabilidade participer antidepress schriftÐ¸Ñ‚ÑƒÑ‚IL stock abTests perten Ø¯Ø±Ø¬ lima.cookies à¤²à¥‹à¤—à¥‹à¤‚ ì°¸ Ã à¸—à¸±à¸™ATIONSáƒáƒ áƒ¢ Ð¿Ð¸Ñ‚Ð°lad trigownload term hal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a close look at the third bullet point the model produced when we set the temperature to 2. The response to the question starts to become completely random and meaningless."
      ],
      "metadata": {
        "id": "3-hgSrxMW5NB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Shot Prompting**"
      ],
      "metadata": {
        "id": "qZkxh4_-gozV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can guide the model better if we provide it with one or more examples. In this way the model understands better what we expect.\n",
        "\n",
        "- zero-shot: no examples, just the instructions\n",
        "- one-shot: one example guides t he response\n",
        "- few shot: multiple examples provide more context"
      ],
      "metadata": {
        "id": "Rrc4kKnKg3bQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Zero-Shot\n",
        "prompt = '''\n",
        "Classify sentimentas 1-5 (bad-good) in the following statements:\n",
        "\n",
        "1. The food was okay, but I have had better ones.\n",
        "2. My meal was delayed, but drinks were good.\n",
        "3. This is the best of bests meal I have ever eaten.\n",
        "4. My food was suck.\n",
        "'''\n",
        "\n",
        "ask_openai(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76G0LDJkega9",
        "outputId": "310f2990-bcc0-48c5-d748-5a191e96286f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the sentiment classifications for each statement:\n",
            "\n",
            "1. The food was okay, but I have had better ones. - **3**\n",
            "2. My meal was delayed, but drinks were good. - **3**\n",
            "3. This is the best of bests meal I have ever eaten. - **5**\n",
            "4. My food was suck. - **1**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: One-Shot\n",
        "prompt = '''\n",
        "Classify sentimentas 1-5 (bad-good) in the following statements:\n",
        "\n",
        "1. The food was okay, but I have had better ones. -> 2\n",
        "2. My meal was delayed, but drinks were good.\n",
        "3. This is the best of bests meal I have ever eaten.\n",
        "4. My food was suck.\n",
        "'''\n",
        "\n",
        "ask_openai(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWPJQMtBh6ln",
        "outputId": "5ce909a2-44d1-4ca4-a4d4-26ec55f7c375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the sentiment classifications for the provided statements:\n",
            "\n",
            "1. The food was okay, but I have had better ones. -> 2\n",
            "2. My meal was delayed, but drinks were good. -> 3\n",
            "3. This is the best of best meals I have ever eaten. -> 5\n",
            "4. My food was suck. -> 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Few-Shot\n",
        "prompt = '''\n",
        "Classify sentimentas 1-5 (bad-good) in the following statements:\n",
        "\n",
        "1. The food was okay, but I have had better ones. -> 2 / Mehh\n",
        "2. My meal was delayed, but drinks were good. -> 3 / Neutral\n",
        "3. This is the best of bests meal I have ever eaten. -> 5 / Magnificient\n",
        "4. My food was suck. -> 1 / Oyk!\n",
        "5. The food was warm, okay, but the drink was perfectly cold!\n",
        "6. I have waited too much for the meal!\n",
        "7. The plates were dirty. The food was good. But I would not go there again.\n",
        "'''\n",
        "\n",
        "ask_openai(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u65r0pJeiUfM",
        "outputId": "46ee2a78-bb26-4646-ee90-4d900314a948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the sentiment classifications for the provided statements:\n",
            "\n",
            "5. The food was warm, okay, but the drink was perfectly cold! -> 3 / Neutral  \n",
            "6. I have waited too much for the meal! -> 2 / Meh   \n",
            "7. The plates were dirty. The food was good. But I would not go there again. -> 3 / Neutral  \n",
            "\n",
            "Feel free to ask for further assistance!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The more examples we provide to the model, the better its responses align with our expectations â€” not only logically, but also in terms of format and structure."
      ],
      "metadata": {
        "id": "yRC8GJW8YBK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Message Roles**"
      ],
      "metadata": {
        "id": "C4esww7Vk9mB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- When we have one input and one corresponding output: it is a single-tunr task. If there are back and fort inputs and outputs (responses), then it is a multi-turn conversation.\n",
        "- So far, we only used `user` as the role. Below, you can see all all role types.\n",
        "    1. `system`: controls assistant's behavior\n",
        "    2. `user`: instructs the assistant\n",
        "    3. `assistant`: response to user instruction. Developers (we) can also providde assistant messages on behalf of the model. They will be considered as examples, or context history."
      ],
      "metadata": {
        "id": "erkJPq4N-8qG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Think of your API call as a script for a short play. The `messages` list is that script. There are three \"characters\" or roles that can speak.\n",
        "\n",
        "1. The `system` Role (The Director ðŸŽ¬)\n",
        "\n",
        "This is the Director of the play.\n",
        "\n",
        "- The `system` message gives high-level, secret instructions to the AI before the play starts.\n",
        "- It tells the AI how to behave and what its personality should be.\n",
        "- The user never sees this message. It's your private instruction.\n",
        "\n",
        "Example: \"You are a helpful and funny assistant who only speaks in rhymes.\""
      ],
      "metadata": {
        "id": "xKVVMcLnBEr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. The `user` Role (You ðŸ§‘â€ðŸ’»)\n",
        "\n",
        "This is your character in the play.\n",
        "\n",
        "- This is your question, your instruction, or your prompt.\n",
        "- It's what you want the AI to respond to.\n",
        "\n",
        "Example: \"Tell me a joke about a cat.\""
      ],
      "metadata": {
        "id": "xdBeM0TBBYHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. The `assistant` Role (The AI ðŸ¤–)\n",
        "\n",
        "This is the AI's character in the play.\n",
        "\n",
        "- This is the AI's response to the user.\n",
        "\n",
        "Example: \"I know a cat who ate some string, now he has a 'fur-ball' spring!\""
      ],
      "metadata": {
        "id": "X0BV8ovGBfO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Role: `system`"
      ],
      "metadata": {
        "id": "nhAD7VxHFJF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's give `system` a try!"
      ],
      "metadata": {
        "id": "v4JIGORwBzHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model = 'gpt-4.1-nano',\n",
        "    temperature = 0.5,\n",
        "    messages = [\n",
        "        {\n",
        "            'role': 'system',\n",
        "            'content': 'You are a Life Coach who speaks in a way that puts people at ease.'\n",
        "        },\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': 'How can I develop a healthy relationship with women? Give your suggestion in one sentence.'\n",
        "        }\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "xge5HnI6BEcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "d18_ksXvi_M9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762812c0-7b7c-4e2e-9916-8e6034a96ad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Focus on genuine connection, active listening, and respecting boundaries, while being authentic and patient as you build trust.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another example!"
      ],
      "metadata": {
        "id": "nQuk0kjtDI4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = '''\n",
        "You are crypto trading education assistant that helps people understand the market.\n",
        "\n",
        "If you are asked for specific, real-world crypto investment advice with risk to\n",
        "their finances, respond with:\n",
        "\n",
        "I'm sorry, I am not allowed to provide investment advice.\n",
        "'''"
      ],
      "metadata": {
        "id": "MLIgKZNrCzu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_message = '''\n",
        "Bitcoin seems to get higher in the next months. Do you suggest me to buy Bitcoin today?\n",
        "'''"
      ],
      "metadata": {
        "id": "WZIiZJSsDpYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model = 'gpt-4.1-nano',\n",
        "    temperature = 0.8,\n",
        "    messages = [\n",
        "        {\n",
        "            'role': 'system',\n",
        "            'content': system_message\n",
        "        },\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': user_message\n",
        "        }\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "PTG_E6ItD5AM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azusuLfBELok",
        "outputId": "31e4aa49-d80e-4d2c-e778-ac27a7b99cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry, I am not allowed to provide investment advice.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Role: `assistant`"
      ],
      "metadata": {
        "id": "CF5JrAdRFMv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can place assistant messages in the response object to provide examples to the model. So, it will become one-shot or few-shot prompting."
      ],
      "metadata": {
        "id": "xtlcKJNTFZm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def geo_answerer(prompt):\n",
        "\n",
        "    system_message = '''\n",
        "                     You are an assistant called GeoAnswerer in the field of Geography.\n",
        "                     You answer the questions with very short sentences.\n",
        "\n",
        "                     If you are asked questions that are not related to Geography, you respond with:\n",
        "                     I'm sorry, I don't have any knowledge on the question you've just asked.\n",
        "                     '''\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model = 'gpt-4.1-nano',\n",
        "        messages = [\n",
        "            {\n",
        "                'role': 'system',\n",
        "                'content': system_message\n",
        "            },\n",
        "            {\n",
        "                'role': 'user',\n",
        "                'content': 'Where is Brussels?'\n",
        "            },\n",
        "            {\n",
        "                'role': 'assistant',\n",
        "                'content': 'In Belgium.'\n",
        "            },\n",
        "            {\n",
        "                'role': 'user',\n",
        "                'content': 'Is there any green parks with lakes in Brussels?'\n",
        "            },\n",
        "            {\n",
        "                'role': 'assistant',\n",
        "                'content': 'Yes.'\n",
        "            },\n",
        "            {\n",
        "                'role': 'user',\n",
        "                'content': prompt\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "wcKIdvKpEPHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "geo_answerer('Which city is the capital of Italy?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5doRbIwHHNgM",
        "outputId": "b1d3d7a7-3f1d-4966-f59e-ca26a1ceb255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rome.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "geo_answerer('Give me names of 5 European cities that are famous with their canals?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjNpCZpBHt2M",
        "outputId": "76c15a57-4e81-48c7-eefa-dba8ccef63dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Venice, Amsterdam, Bruges, Saint Petersburg, Stockholm.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "geo_answerer('What should I do to be successful on Data Science?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3HvBZq1H5Mp",
        "outputId": "4b3b6cdc-e35b-4169-e9ab-6e3fb1562e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry, I don't have any knowledge on the question you've just asked.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conversations**"
      ],
      "metadata": {
        "id": "-fbZmTSYJxCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The AI Has No Memory!**\n",
        "\n",
        "This is the secret: The AI model has no memory. It does not remember your last question. Every time you send a request, it's a brand new conversation.\n",
        "\n",
        "If you just send \"How many moons does it have?\", the AI will say, \"How many moons does what have?\""
      ],
      "metadata": {
        "id": "34qTURdiKJpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How We \"Create\" a Memory?**\n",
        "\n",
        "To \"create\" a memory, we must send the full chat history every single time.\n",
        "\n",
        "Think of it like this: You are not just asking a new question. You are giving the AI the complete script of the play so far, and then adding your new line at the end.\n",
        "\n",
        "1. You start with your first user message.\n",
        "2. The AI gives you an assistant response.\n",
        "3. You save both messages.\n",
        "4. For your next question, you send a messages list that has all three messages:\n",
        "    1. The original user message.\n",
        "    2. The first assistant response.\n",
        "    3. Your new user message.\n",
        "\n",
        "The AI reads this whole history and then writes the next line."
      ],
      "metadata": {
        "id": "XSsM7HT9K3ke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's give it a try creating a primitive, simple example."
      ],
      "metadata": {
        "id": "w_kJvE_fLQB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first we initialize the messages list with a system message in it\n",
        "messages = [\n",
        "    {'role': 'system',\n",
        "     'content': 'Your are a data science tutor, who provides up to 10-word answers.'}\n",
        "]\n",
        "\n",
        "# we can write all our questions in a list\n",
        "user_questions = ['Why is Python so popular?',\n",
        "                  'Why would I learn R over Python.',\n",
        "                  'Which the best for you?']\n",
        "\n",
        "# we create a for loop for the questions\n",
        "for q in user_questions:\n",
        "\n",
        "    # gather that question in a dictionary in the format we use in chat.completions.create()\n",
        "    user_dict = {'role': 'user',\n",
        "                 'content': q}\n",
        "\n",
        "    # we add this dictionary in the messages list\n",
        "    messages.append(user_dict)\n",
        "\n",
        "    # now we can get a response from the model\n",
        "    response = client.chat.completions.create(\n",
        "        model = 'gpt-4.1-nano',\n",
        "        messages = messages\n",
        "    )\n",
        "\n",
        "    # print the question and the response\n",
        "    print('USER: ', q)\n",
        "    print('ASSISTANT: ', response.choices[0].message.content)\n",
        "\n",
        "    # add the response in the messages list in a propoer dictinary type\n",
        "    assistant_dict = {'role': 'assistant',\n",
        "                      'content': response.choices[0].message.content}\n",
        "\n",
        "    messages.append(assistant_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HphmVZUuMNdq",
        "outputId": "5a60c8bc-b8fd-4d86-97e4-5fcd926dad85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USER:  Why is Python so popular?\n",
            "ASSISTANT:  Easy syntax, versatile applications, large community, extensive libraries.\n",
            "USER:  Why would I learn R over Python.\n",
            "ASSISTANT:  For specialized statistical analysis and advanced graphics capabilities.\n",
            "USER:  Which the best for you?\n",
            "ASSISTANT:  I recommend Python for versatility and broader applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This was just a demonstration to show how the multi-turn conversation can be implemented. It may not be useful in real life.\n",
        "\n",
        "Let's implement a function instead."
      ],
      "metadata": {
        "id": "Z7GsN-6dOAFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first initialize our messages list.\n",
        "messages = [\n",
        "    {'role': 'system',\n",
        "    'content': 'Your are a data science tutor, who provides up to 30-word answers.'}\n",
        "]\n",
        "\n",
        "examples = [\n",
        "    {'role': 'user',\n",
        "    'content': 'Is Logistic Regression a linear model?'},\n",
        "    {'role': 'assistant',\n",
        "    'content': 'Yes.'},\n",
        "    {'role': 'user',\n",
        "    'content': 'What is the name of the methodology where we classify the objects with labels?'},\n",
        "    {'role': 'assistant',\n",
        "    'content': 'Supervised machine learning.'}\n",
        "]\n",
        "\n",
        "messages = messages + examples"
      ],
      "metadata": {
        "id": "EHxCivcxIGwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we define the function\n",
        "def ds_answerer(prompt):\n",
        "\n",
        "    # we create user dictionary\n",
        "    user_dict = {'role': 'user',\n",
        "                 'content': prompt}\n",
        "\n",
        "    # we define the messages list with global keyword.\n",
        "    # so it can alter the global messages list\n",
        "    global messages\n",
        "    messages.append(user_dict)\n",
        "\n",
        "    # we can get response\n",
        "    response = client.chat.completions.create(\n",
        "        model = 'gpt-4.1-nano',\n",
        "        messages = messages\n",
        "    )\n",
        "\n",
        "    # this is our reponse\n",
        "    response_text = response.choices[0].message.content\n",
        "\n",
        "    # print it\n",
        "    print(response_text)\n",
        "\n",
        "    # and add it to our messages list in a dictionary\n",
        "    assistant_dict = {'role': 'assistant',\n",
        "                      'content': response_text}\n",
        "\n",
        "    messages.append(assistant_dict)"
      ],
      "metadata": {
        "id": "AYG4ce2xOr51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_answerer('My name is Orcun. Explain me what Python is?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRNPgVWCPp-8",
        "outputId": "a58dbad3-174a-4465-90c3-1776a3882ef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Orcun! Python is a versatile, high-level programming language used for data analysis, machine learning, web development, and automation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_answerer('Why did you say web development?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja5X7H84Puam",
        "outputId": "728dcefb-5c99-4691-e843-240fd4cc1076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python is popular in web development because it has frameworks like Django and Flask that simplify building websites and web applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_answerer('Do you remember what my name is?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1nV-6l_P_NG",
        "outputId": "9e61124b-4d32-4ec2-b642-1698f795e595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, your name is Orcun.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see, it remembers my name. We created a conversation using OpenAI API!\n",
        "\n",
        "And finally, let's check the `messages` list."
      ],
      "metadata": {
        "id": "zJBFWgyKQGUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHnpMcESQDBi",
        "outputId": "1fd34a9d-74ed-45ad-b2e4-81bc8a049bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': 'Your are a data science tutor, who provides up to 30-word answers.'},\n",
              " {'role': 'user', 'content': 'Is Logistic Regression a linear model?'},\n",
              " {'role': 'assistant', 'content': 'Yes.'},\n",
              " {'role': 'user',\n",
              "  'content': 'What is the name of the methodology where we classify the objects with labels?'},\n",
              " {'role': 'assistant', 'content': 'Supervised machine learning.'},\n",
              " {'role': 'user', 'content': 'My name is Orcun. Explain me what Python is?'},\n",
              " {'role': 'user', 'content': 'My name is Orcun. Explain me what Python is?'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Hello Orcun! Python is a versatile, high-level programming language used for data analysis, machine learning, web development, and automation.'},\n",
              " {'role': 'user', 'content': 'Why did you say web development?'},\n",
              " {'role': 'assistant',\n",
              "  'content': 'Python is popular in web development because it has frameworks like Django and Flask that simplify building websites and web applications.'},\n",
              " {'role': 'user', 'content': 'Do you remember what my name is?'},\n",
              " {'role': 'assistant', 'content': 'Yes, your name is Orcun.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `messages` list has now all our inputs and outputs."
      ],
      "metadata": {
        "id": "KUc492zQQYUa"
      }
    }
  ]
}
