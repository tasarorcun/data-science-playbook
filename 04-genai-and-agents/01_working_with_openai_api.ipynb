{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "166a37a3",
   "metadata": {},
   "source": [
    "**Colab Execution:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tasarorcun/data-science-playbook/blob/main/04-genai-and-agents/001_working_with_openai_api.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6317d0b",
   "metadata": {},
   "source": [
    "# 0. Import Generic Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c509466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore notebook warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# os\n",
    "import os\n",
    "\n",
    "# sys\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4aaed7",
   "metadata": {},
   "source": [
    "# 1. OpenAI API Integration Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51535302",
   "metadata": {},
   "source": [
    "**Objective:** Establish a robust, programmatic connection to OpenAI's inference engine for clinical and operational workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085c28af",
   "metadata": {},
   "source": [
    "## 1.1 Architectural Context: GUI vs. API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd19c35",
   "metadata": {},
   "source": [
    "While browser-based interfaces (like ChatGPT) function as **SaaS** (Software as a Service) for human interaction, they lack the scalability required for engineering tasks.\n",
    "\n",
    "This notebook focuses on the **OpenAI API**, which provides a **RESTful interface** to:\n",
    "* **Automate Workflows:** Process bulk datasets (e.g., 1,000+ clinical summaries) without manual input.\n",
    "* **Integrate Backend Logic:** Embed LLM capabilities directly into proprietary web applications or data pipelines.\n",
    "* **Maintain Statelessness:** Interact with models programmatically, where each request is independent and configurable.\n",
    "\n",
    "> **Note:** Unlike local model deployment, the API delegates computational load to OpenAI's infrastructure, requiring strict management of **Authentication** and **Latency**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d11f70",
   "metadata": {},
   "source": [
    "## 1.2 Credential Management & Security Protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1992a96e",
   "metadata": {},
   "source": [
    "Access to the inference engine requires a unique **Secret API Key**. This key authenticates requests and tracks token usage for billing purposes.\n",
    "\n",
    "**Generation Workflow:**\n",
    "1.  Navigate to the [OpenAI Platform Dashboard](https://platform.openai.com/api-keys).\n",
    "2.  Generate a new secret key (`sk-...`).\n",
    "3.  **Storage:** The key is displayed only once. It must be stored immediately in a secure credential manager (e.g., 1Password, Vault).\n",
    "\n",
    "### Secrets Management Strategy\n",
    "Hardcoding credentials (e.g., `api_key = \"sk-...\"`) directly into source code is a **critical security vulnerability**, particularly in collaborative environments using Version Control (Git).\n",
    "\n",
    "**Production Standards:**\n",
    "* **Dynamic Loading:** Credentials should be injected via **OS Environment Variables** or restricted local configuration files (`.env` / `.txt`).\n",
    "* **Version Control:** Ensure all credential files are explicitly listed in `.gitignore` to prevent accidental leakage to public repositories.\n",
    "* **Key Rotation:** Implement regular key rotation policies to minimize impact in case of compromise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c6e7ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_api_key(filepath: str) -> str:\n",
    "\n",
    "    '''\n",
    "    Process:\n",
    "        Seccurely loads the API key from a local file and injects it into the environment.\n",
    "    Args:\n",
    "        filepath (str): Relative path to the secret key file.\n",
    "    Returns:\n",
    "        str: The loaded API key (masked).\n",
    "    Raises:\n",
    "        FileNotFoundError: If the credential file is missing.\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        with open(filepath, 'r') as file:\n",
    "            api_key = file.read().strip()\n",
    "\n",
    "            # Simple validation\n",
    "            if not api_key.startswith('sk-'):\n",
    "                raise ValueError('Invalid API key format. Key must start with \"sk-\".')\n",
    "\n",
    "            # Set as Environment Variable for implicit client authentication\n",
    "            os.environ['OPENAI_API_KEY'] = api_key\n",
    "\n",
    "            # Security: Never print the full key in outputs\n",
    "            masked_key = f'{api_key[:4]}...{api_key[-4:]}'\n",
    "            print(f'Authentication successful. Key loaded: {masked_key}')\n",
    "            return api_key\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f'Credential file not found at {filepath}')\n",
    "        print(f'Hint: Ensure \"openai_api.txt\" exists in the \"api_keys\" directory.')\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ac8d6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful. Key loaded: sk-p...nJoA\n"
     ]
    }
   ],
   "source": [
    "# Execution of the load_api_key(function)\n",
    "key_path = os.path.join('.', 'api_keys', 'openai_api.txt')\n",
    "_ = load_api_key(key_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6153d",
   "metadata": {},
   "source": [
    "## 1.3 Client Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c52850",
   "metadata": {},
   "source": [
    "We instantiate the `OpenAI` client to act as the interface gateway.\n",
    "\n",
    "**Note on Authentication:**\n",
    "Since we injected `OPENAI_API_KEY` into the environment variables in the previous step, the client automatically detects credentials without explicit argument passing. This enforces a **clean separation of concerns**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1fe3107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established successfully.\n",
      "Endpoint reachable. First available model: gpt-4-0613\n"
     ]
    }
   ],
   "source": [
    "# import OpenAI\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize client\n",
    "# The liobary automatically looks for \"OPENAI_API_KEY\" in os.environ\n",
    "client = OpenAI()\n",
    "\n",
    "# Connection test\n",
    "# A simple API call to verify the authentication immediately\n",
    "try:\n",
    "    models = client.models.list()\n",
    "    first_model = models.data[0].id\n",
    "    print('Connection established successfully.')\n",
    "    print(f'Endpoint reachable. First available model: {first_model}')\n",
    "except Exception as e:\n",
    "    print(f'Connection failed: {str(e)}')\n",
    "    print('Chheck your API key and internet connection!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea22ab3",
   "metadata": {},
   "source": [
    "## 1.4 Basic Interaction Pattern (Stateless)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112fcdf9",
   "metadata": {},
   "source": [
    "The core interaction method is `client.chat.completions.create`. The API operates on a **Stateless Protocol**, meaning each request is isolated and retains no memory of previous interactions.\n",
    "\n",
    "### SDK Method Anatomy\n",
    "\n",
    "Understanding the Python client structure ensures correct resource targeting:\n",
    "\n",
    "* **`client`**: The authenticated instance managing connection pooling and configuration.\n",
    "* **`.chat`**: The namespace targeting chat-based Large Language Models (distinct from `.images` or `.audio`).\n",
    "* **`.completions`**: The specific resource group handling text generation tasks.\n",
    "* **`.create()`**: The execution method that triggers the **Synchronous HTTP POST** request to the API endpoint.\n",
    "\n",
    "### Key Request Parameters\n",
    "* **`model`**: The inference engine ID (e.g., `gpt-4o-mini` for latency-sensitive tasks).\n",
    "* **`messages`**: An array of message objects defining the conversation history.\n",
    "* **`role`**: Defines the entity speaking (`system`, `user`, or `assistant`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0174a547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: The OpenAI API is a cloud-based service that allows developers to integrate advanced artificial intelligence models, such as language processing and generation capabilities, into their applications.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model = 'gpt-4o-mini',\n",
    "        messages = [\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': 'What is the OpenAI API? Explain in one sentence.'\n",
    "            }\n",
    "        ],\n",
    "        temperature = 0\n",
    "    )\n",
    "\n",
    "    # Extract payload content\n",
    "    print(f'Output: {response.choices[0].message.content}')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Request failed: {str(e)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0094c1d",
   "metadata": {},
   "source": [
    "## 1.5 Functional Abstraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0bc233",
   "metadata": {},
   "source": [
    "Direct API calls require repetitive boilerplate code (client initialization, message formatting, parameter tuning).\n",
    "\n",
    "To adhere to the **DRY (Don't Repeat Yourself)** principle, we encapsulate this logic into a reusable function. This abstraction layer allows us to:\n",
    "1.  **Standardize Parameters:** Enforce default settings (e.g., `temperature=0` for clinical consistency).\n",
    "2.  **Simplify Interfaces:** Reduce the API surface area to a single function call.\n",
    "3.  **Decouple Logic:** Easily swap models (`gpt-4o` vs `gpt-3.5`) without rewriting downstream code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2a48180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clinical_response(prompt: str, model: str = 'gpt-4o-mini', temperature: float = 0.8) -> str:\n",
    "\n",
    "    '''\n",
    "    Process:\n",
    "        Executes a prompt against the OpenAI API and returns the extracted text.\n",
    "    Args:\n",
    "        prompt (str): The user query or instruction.\n",
    "        model (str): Target model ID. Default is \"gpt-4o-mini\"\n",
    "        temperature (float): Randomness of the response. Default is 0.8.\n",
    "    Returns:\n",
    "        str: The generated response text.\n",
    "    '''\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = [\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': prompt\n",
    "            }\n",
    "        ],\n",
    "        temperature = temperature\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7da96cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real-World Data (RWD) refers to the data collected from various sources outside of traditional clinical trials, such as electronic health records, claims data, and patient registries. Real-World Evidence (RWE) is the analysis and interpretation of this data to assess the effectiveness, safety, and value of medical interventions and treatments in everyday clinical settings.\n"
     ]
    }
   ],
   "source": [
    "# Domain specific example\n",
    "user_prompt = 'Explain me in two sentences: what RWD and RWE are in the field of healthcare.'\n",
    "print(get_clinical_response(prompt = user_prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d517c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Generation Sequencing (NGS) technologies began to emerge in the mid-2000s. The first commercial NGS instruments were introduced around 2005, with the release of platforms such as the 454 Life Sciences Genome Sequencer. This marked a significant advancement in sequencing technology, allowing for rapid and high-throughput sequencing of DNA.\n"
     ]
    }
   ],
   "source": [
    "# Another domain specific example\n",
    "user_prompt = 'What was the year Next Generation Sequencing first released?'\n",
    "print(get_clinical_response(prompt = user_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e04ee",
   "metadata": {},
   "source": [
    "# 2. Tokenomics & Cost Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced904f",
   "metadata": {},
   "source": [
    "In large-scale ETL pipelines (e.g., processing 500,000+ Electronic Health Records), API costs become a critical architectural constraint.\n",
    "\n",
    "**Cost Dynamics:**\n",
    "* **Asymmetry:** For `gpt-4o-mini`, output tokens are **4x more expensive** than input tokens ($0.60 vs $0.15 per 1M).\n",
    "* **Optimization Strategy:**\n",
    "    1.  **Input:** Use concise context; trim unnecessary whitespace/boilerplate.\n",
    "    2.  **Output:** Enforce strict output schemas (JSON) to prevent the model from \"yapping\" (generating verbose, non-billable filler text).\n",
    "\n",
    "The following script estimates the cost of a single transaction to forecast batch processing budgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6430df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define model pricing (per 1M tokens) - Source: OpenAI pricing page\n",
    "model_pricing = {\n",
    "    'gpt-4o-mini': {'input': 0.15, 'output': 0.60},\n",
    "    'gpt-4o': {'input': 2.50, 'output': 10.00}\n",
    "}\n",
    "\n",
    "# Create function\n",
    "def calculate_request_cost(usage, model: str = 'gpt-4o-mini') -> float:\n",
    "\n",
    "    '''\n",
    "    Process:\n",
    "        Calculates the precise cost of an API call based on current pricing.\n",
    "    Args:\n",
    "        usage (openai.types.completion_usage.CompletionUsage): OpenAI object holding used token numbers.\n",
    "        model (str): OpenAI model ID.\n",
    "    Returns:\n",
    "        float: Total price (price for input + price for output).\n",
    "    '''\n",
    "\n",
    "    # get the tariff\n",
    "    prices = model_pricing.get(model, model_pricing['gpt-4o-mini'])\n",
    "\n",
    "    # calculate price for input & output tokens\n",
    "    input_price = (usage.prompt_tokens / 1_000_000) * prices['input']\n",
    "    output_price = (usage.completion_tokens / 1_000_000) * prices['output']\n",
    "\n",
    "    return input_price + output_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1adcacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response text:\n",
      "- Mild gastrointestinal discomfort\n",
      "- Nausea\n",
      "Number of input tokens: 75\n",
      "Number of output tokens: 9\n",
      "Single response cost: $0.00027750\n",
      "Projected cost for 1M responses: $277.50\n"
     ]
    }
   ],
   "source": [
    "# Execution\n",
    "clinical_prompt = '''\n",
    "Patient Report: 45-year-old male initiated on Metformin 500mg BID. \n",
    "Follow-up at 2 weeks: Patient reports mild gastrointestinal discomfort and nausea. \n",
    "No signs of lactic acidosis. Vitals stable.\n",
    "\n",
    "Task: Extract any Adverse Drug Reactions (ADRs) mentioned. \n",
    "Format: List only.\n",
    "'''\n",
    "\n",
    "# Get response\n",
    "response = client.chat.completions.create(\n",
    "    model = 'gpt-4o-mini',\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': clinical_prompt\n",
    "        }\n",
    "    ],\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "# Cost analysis\n",
    "total_cost = calculate_request_cost(usage = response.usage,\n",
    "                                    model = 'gpt-4o')\n",
    "\n",
    "# Prints\n",
    "print(f'Response text:\\n{response.choices[0].message.content}')\n",
    "print(f'Number of input tokens: {response.usage.prompt_tokens}')\n",
    "print(f'Number of output tokens: {response.usage.completion_tokens}')\n",
    "print(f'Single response cost: ${total_cost:.8f}')\n",
    "print(f'Projected cost for 1M responses: ${total_cost * 1_000_000:,.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d36136",
   "metadata": {},
   "source": [
    "# 3. Parameter Tuning: Control of Stochasticity & Randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384c5dab",
   "metadata": {},
   "source": [
    "The `temperature` parameter (0.0 to 2.0) controls the entropy of the token sampling distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cee06d",
   "metadata": {},
   "source": [
    "### Clinical Strategy\n",
    "* **Deterministic ($T=0$):** Forces the model to select the highest probability token.\n",
    "    * *Use Case:* Data Extraction, coding (ICD-10), structured JSON output.\n",
    "* **Stochastic ($T > 0.7$):** Flattens the probability curve, introducing variance.\n",
    "    * *Use Case:* Patient communication drafts, synthetic data generation, brainstorming.\n",
    "\n",
    "> **Warning:** In GxP (Good Practice) environments, reproducibility is key. Unless generating creative content, always default to `temperature=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599596f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_temperature(prompt: str, temp: float, iterations: int = 3):\n",
    "\n",
    "    '''\n",
    "    Process:\n",
    "        Executes a prompt multiple times to empirically validate the impact of temperature on output stability.\n",
    "        Serves as a unit test for 'Reproducibility'. It verifies whether\n",
    "        the model behaves deterministically (at T=0) or introduces variance (at T>0.7),\n",
    "        which is critical for validating GxP-compliant workflows.\n",
    "    Args:\n",
    "        prompt (str): The input query to be tested.\n",
    "        temp (float): The sampling temperature (0.0 = deterministic, 2.0 = highly stochastic).\n",
    "        iterations (int): Number of sequential API calls to perform (Default: 3).\n",
    "    '''\n",
    "\n",
    "    print(f'##### Testing temperature: {temp}')\n",
    "    print('-' * 20)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        response = client.chat.completions.create(\n",
    "            model = 'gpt-4o-mini',\n",
    "            temperature = temp,\n",
    "            messages = [\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        print(f'Iteration {i + 1}: {response.choices[0].message.content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d96d69",
   "metadata": {},
   "source": [
    "Execution of the test function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce0f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Testing temperature: 0.0\n",
      "--------------------\n",
      "Iteration 1: \"...target identification and validation.\"\n",
      "Iteration 2: \"...target identification and validation.\"\n",
      "Iteration 3: \"...target identification and validation.\"\n",
      "##### Testing temperature: 2.0\n",
      "--------------------\n",
      "Iteration 1: \"target selection and verification.\"\n",
      "Iteration 2: \"target identification and validation.\"\n",
      "Iteration 3: \"...understanding disease mechanisms accurately.\"\n"
     ]
    }
   ],
   "source": [
    "test_prompt = 'Complete this sentence in less than 5 words: \"The most critical factor in drug discovery is...\"'\n",
    "\n",
    "# 1. Deterministic test (Stability - Less Variance)\n",
    "test_temperature(test_prompt, temp = 0.0)\n",
    "\n",
    "# 2. Stochastic test (Creativity - High Variance)\n",
    "test_temperature(test_prompt, temp = 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9541b02",
   "metadata": {},
   "source": [
    "# 4. Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2f1288",
   "metadata": {},
   "source": [
    "LLMs perform significantly better when provided with exemplar pairs (Input -> Output) within the context window. This technique, known as **In-Context Learning**, steers the model's behavior without the need for parameter fine-tuning.\n",
    "\n",
    "### Strategies\n",
    "* **Zero-Shot:** Direct instruction with no prior examples. Useful for general knowledge tasks.\n",
    "* **One-Shot:** Providing a single example to establish output format structure.\n",
    "* **Few-Shot:** Providing multiple examples (3-5) to teach complex logic, nuance, and domain-specific taxonomy.\n",
    "\n",
    "> **Use Case:** Classifying adverse events in unstructured clinical notes into standard toxicity grades (e.g., Mild, Moderate, Severe).\n",
    ">\n",
    "> **Scenario:** Classifying Patient Feedback into Toxicity Grades (1-3)<br>\n",
    "> - 1: Mild (Does not interfere with daily activities)<br>\n",
    "> - 2: Moderate (Interferes with daily activities)<br>\n",
    "> - 3: Severe (Requires medical intervention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f34d56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##--- Zero-Shot Output ---##\n",
      "Based on the provided patient reports, here is the classification of severity:\n",
      "\n",
      "1. \"I feel a bit dizzy when I stand up fast.\" - **Grade 1 (Mild)**\n",
      "2. \"The nausea is so bad I can't go to work.\" - **Grade 3 (Severe)**\n",
      "3. \"My skin is peeling off and it burns like fire.\" - **Grade 3 (Severe)**\n",
      "4. \"Just a slight headache.\" - **Grade 1 (Mild)**\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Zero-Shot Prompting (No Guidance) ---\n",
    "\n",
    "prompt_zero = '''\n",
    "Classify the severity (Grade 1-3, (Mild, Moderate, Severe)) of the following patient reports:\n",
    "\n",
    "1. \"I feel a bit dizzy when I stand up fast.\"\n",
    "2. \"The nausea is so bad I can't go to work.\"\n",
    "3. \"My skin is peeling off and it burns like fire.\"\n",
    "4. \"Just a slight headache.\"\n",
    "'''\n",
    "\n",
    "print('##--- Zero-Shot Output ---##')\n",
    "print(get_clinical_response(prompt_zero, temperature = 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51fe14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##--- Few-Shot Output ---##\n",
      "Here are the classifications based on the provided examples:\n",
      "\n",
      "1. \"I feel a bit dizzy when I stand up fast.\" -> Grade 1 (No interference)\n",
      "2. \"The nausea is so bad I can't go to work.\" -> Grade 2 (Interferes with activity)\n",
      "3. \"My skin is peeling off and it burns like fire.\" -> Grade 3 (Medical intervention)\n",
      "4. \"Just a slight headache.\" -> Grade 1 (No interference)\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Few-Shot Prompting (Pattern Injection) ---\n",
    "\n",
    "prompt_few = '''\n",
    "Classify the severity (Grade 1-3) of the patient reports based on functional impairment.\n",
    "\n",
    "Examples:\n",
    "- \"I have a runny nose but I'm fine.\" -> Grade 1 (No interference)\n",
    "- \"I threw up all morning and missed my appointment.\" -> Grade 2 (Interferes with activity)\n",
    "- \"I had to go to the ER because I couldn't breathe.\" -> Grade 3 (Medical intervention)\n",
    "\n",
    "Now classify these:\n",
    "1. \"I feel a bit dizzy when I stand up fast.\"\n",
    "2. \"The nausea is so bad I can't go to work.\"\n",
    "3. \"My skin is peeling off and it burns like fire.\"\n",
    "4. \"Just a slight headache.\"\n",
    "'''\n",
    "\n",
    "print('##--- Few-Shot Output ---##')\n",
    "print(get_clinical_response(prompt_few, temperature = 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "595d47b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##--- One-Shot Output ---##\n",
      "```json\n",
      "{\"drug\": \"Lipitor\", \"strength\": \"20mg\", \"frequency\": \"QHS\"}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# --- 3. One-Shot Prompting (Schema Enforcement) ---\n",
    "\n",
    "# Use Case: We don't need to teach the model 'medicine',\n",
    "# we just need to enforce a strict JSON format.\n",
    "\n",
    "prompt_one = '''\n",
    "Extract medication entities into a JSON object with keys: 'drug', 'strength', 'frequency'.\n",
    "\n",
    "Example:\n",
    "Input: \"Patient takes 500mg Tylenol twice a day.\"\n",
    "Output: {\"drug\": \"Tylenol\", \"strength\": \"500mg\", \"frequency\": \"BID\"}\n",
    "\n",
    "Task:\n",
    "Input: \"Prescribed Lipitor 20mg every evening for cholesterol.\"\n",
    "Output:\n",
    "'''\n",
    "\n",
    "print('##--- One-Shot Output ---##')\n",
    "print(get_clinical_response(prompt_one, temperature = 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ca7552",
   "metadata": {},
   "source": [
    "# 5. Context Architecture: Role Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8c222b",
   "metadata": {},
   "source": [
    "Effective prompt engineering relies on the structured interaction between three distinct roles within the `messages` array. Understanding these roles is critical for designing **Stateless** (Single-turn) vs. **Stateful** (Multi-turn) systems.\n",
    "\n",
    "### The Roles Schema\n",
    "\n",
    "* **1. System (`system`): Global Behavior & Guardrails**\n",
    "    * Sets the metaprompt that persists throughout the session.\n",
    "    * Defines the persona, output format, and safety constraints (e.g., *\"You are a Clinical Decision Support System. Do not provide medical diagnoses.\"*).\n",
    "    * *Engineering Note:* The system prompt is weighted heavily by the model to steer overall behavior.\n",
    "\n",
    "* **2. User (`user`): The Signal**\n",
    "    * Represents the external input, query, or instruction triggering the inference.\n",
    "\n",
    "* **3. Assistant (`assistant`): Memory & Pattern Injection**\n",
    "    * **Primary Use:** Stores prior model responses to simulate \"Memory\" in a conversational application.\n",
    "    * **Advanced Use (Few-Shot):** Developers can \"fake\" assistant messages to provide ideal response examples (Input -> Output pairs) within the context window, without fine-tuning the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441a24fd",
   "metadata": {},
   "source": [
    "## 5.1 The System Role: Guardrails & Persona"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b866db44",
   "metadata": {},
   "source": [
    "The `system` message is the most powerful lever for steering model behavior. In Life Sciences applications, it could be primarily used for:\n",
    "\n",
    "1.  **Tone Calibration:** Adjusting the complexity and empathy level based on the audience (e.g., Patient vs. Clinician).\n",
    "2.  **Regulatory Guardrails:** Explicitly forbidding the model from performing restricted actions, such as making diagnoses or prescribing medication.\n",
    "\n",
    "> **Engineering Note:** Strong system prompts are essential for **GxP compliance**, ensuring the AI does not hallucinate medical credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b0788a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##--- Scenario A: Tone Calibration - Empathetic Explanation (Patient-Centric) ---##\n",
      "Patient: The doctor said I have \"Idiopathic Pulmonary Fibrosis\". It sounds scary. What is it?\n",
      "Bot: Idiopathic Pulmonary Fibrosis is a condition where the lungs become stiff and make it harder to breathe, but it’s important to know that you’re not alone, and there are ways to manage it. Your doctor will work with you to find the best treatment options to help you feel better.\n"
     ]
    }
   ],
   "source": [
    "# --- Scenario A: Tone Calibration (Patient-Centric) ---\n",
    "# Goal: Explain a complex concept to a patient with empathy\n",
    "\n",
    "system_persona = '''\n",
    "You are an empathetic Clinical Care Coordinator. \n",
    "Your goal is to explain medical concepts to patients in simple, reassuring language.\n",
    "Avoid technical jargon.\n",
    "You explanation should not exceed 2 sentences.\n",
    "'''\n",
    "\n",
    "user_query = 'The doctor said I have \"Idiopathic Pulmonary Fibrosis\". It sounds scary. What is it?'\n",
    "\n",
    "print('##--- Scenario A: Tone Calibration - Empathetic Explanation (Patient-Centric) ---##')\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = 'gpt-4o-mini',\n",
    "    temperature = 0.8, # for slighlty natural, hhuman tone\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': system_persona\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': user_query\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f'Patient: {user_query}')\n",
    "print(f'Bot: {response.choices[0].message.content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fe7b5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##--- Scenario B: Safety Boundary ---##\n",
      "Patient: I have a sharp pain in my left chest. Should I take Aspirin?\n",
      "Bot: I cannot provide medical advice. Please consult your physician.\n"
     ]
    }
   ],
   "source": [
    "# --- Scenario B: Regulatory Guardrails (Safety) ---\n",
    "# Goal: Prevent the model from giving specific medical advice.\n",
    "\n",
    "system_guardrail = '''\n",
    "You are a Medical Information Assistant. \n",
    "You provide information based on package inserts.\n",
    "CRITICAL RULE: You are NOT a doctor. DO NOT provide diagnosis or treatment advice. \n",
    "If asked for advice, respond with standard disclaimer:\n",
    "\"I cannot provide medical advice. Please consult your physician.\"\n",
    "'''\n",
    "\n",
    "risky_query = 'I have a sharp pain in my left chest. Should I take Aspirin?'\n",
    "\n",
    "print('##--- Scenario B: Safety Boundary ---##')\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = 'gpt-4o-mini',\n",
    "    temperature = 0.8,\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': system_guardrail\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': risky_query\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f'Patient: {risky_query}')\n",
    "print(f'Bot: {response.choices[0].message.content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb0cada",
   "metadata": {},
   "source": [
    "## 5.2 The Assistant Role: Few-Shot Pattern Injection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e00bb8",
   "metadata": {},
   "source": [
    "While the `assistant` role is primarily used for storing conversation history, it is also a powerful tool for **In-Context Learning**.\n",
    "\n",
    "By pre-populating the context window with \"fake\" User-Assistant pairs, we can steer the model's output format and logic without explicit instructions. This technique could be more effective than complex System Prompts for enforcing **standardized nomenclature** (e.g., mapping symptoms to ICD-10 codes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4cc5629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_diagnosis(raw_input: str) -> str:\n",
    "\n",
    "    '''\n",
    "    Process:\n",
    "        Maps unstructured clinical notes to standardized ICD-10 codes\n",
    "        using Few-Shot examples injected via the \"assistant\" role.\n",
    "    Args:\n",
    "        raw_input (str): user query.\n",
    "    Returns:\n",
    "        str: response message content.\n",
    "    '''\n",
    "\n",
    "    # 1. System: Define the task\n",
    "    system_injection = '''\n",
    "    You are a Medical Coding Assistant.\n",
    "    Map the input description to the closest standard ICD-10 name and code.\n",
    "    Output ONLY the code and name.\n",
    "    '''\n",
    "\n",
    "    # 2. Few-Shot history\n",
    "    few_shot_examples = [\n",
    "        {\"role\": \"user\", \"content\": \"pt complains of chest pain\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Chest pain, unspecified (R07.9)\"},\n",
    "        \n",
    "        {\"role\": \"user\", \"content\": \"type 2 diabetes with kidney issues\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Type 2 diabetes mellitus with kidney complications (E11.2)\"},\n",
    "        \n",
    "        {\"role\": \"user\", \"content\": \"high bp\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Essential (primary) hypertension (I10)\"}\n",
    "    ]\n",
    "\n",
    "    # 3. Construct thhe final payload\n",
    "    messages = [{'role': 'system', 'content': system_injection}]\n",
    "    messages.extend(few_shot_examples)\n",
    "    messages.append({'role': 'user', 'content': raw_input})\n",
    "\n",
    "    # 4. Execute\n",
    "    response = client.chat.completions.create(\n",
    "        model = 'gpt-4o-mini',\n",
    "        temperature = 0, # this task requires deterministic output\n",
    "        messages = messages\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "420a91ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \"patient has a really bad headache\"\n",
      "Output: Headache, unspecified (R51)\n",
      "\n",
      "Input: \"feeling of heart racing\"\n",
      "Output: Palpitations (R00.2)\n"
     ]
    }
   ],
   "source": [
    "# --- Test cases ---\n",
    "print(f'Input: \"patient has a really bad headache\"')\n",
    "print(f'Output: {normalize_diagnosis(\"patient has a really bad headache\")}\\n')\n",
    "\n",
    "print(f'Input: \"feeling of heart racing\"')\n",
    "print(f'Output: {normalize_diagnosis(\"feeling of heart racing\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4644b52",
   "metadata": {},
   "source": [
    "# 6. State Management: Handling Conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66b1007",
   "metadata": {},
   "source": [
    "The OpenAI API is **stateless**. If a follow-up query about a patient mentioned in a previous request is sent, the model will not \"remember\" the patient unless you explicitely re-send the entire conversation history.\n",
    "\n",
    "In **RWD (Real World Data)** applications, constructing a patient's timeline (Longitudinal History) requires maintaining a persistent `messages` list (Context Window) that grows with each interaction.\n",
    "\n",
    "> **Note:** As the conversation grows, we consume more input tokens per request. In production, strategies like **\"Context Window Sliding\"** or **\"Summarization\"** are required to stay within token limits (e.g., 128k context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e6a2c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- The Mechanics of State Management ---\n",
    "\n",
    "# 1. Initialize an empty session (The Electronic Health Record - EHR Buffer)\n",
    "\n",
    "messages_buffer = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are a Clinical Case Summarizer. Maintain a professional tone.'\n",
    "    }\n",
    "]\n",
    "\n",
    "def update_and_chat(new_input: str):\n",
    "\n",
    "    '''\n",
    "    Process:\n",
    "        Manually appends new input to the buffer.\n",
    "        Sends the FULL buffer to the API.\n",
    "        Appends the response back to the buffer.\n",
    "    Args:\n",
    "        new_input (str): New user input / query / prompt.\n",
    "    '''\n",
    "\n",
    "    # Append user input\n",
    "    messages_buffer.append({'role': 'user', 'content': new_input})\n",
    "\n",
    "    # Send thhe WHOLE history to the API\n",
    "    response = client.chat.completions.create(\n",
    "        model = 'gpt-4o-mini',\n",
    "        temperature = 0,\n",
    "        messages = messages_buffer\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    # Append assistant response\n",
    "    messages_buffer.append({'role': 'assistant', 'content': answer})\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "500c48c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##--- Step 1: Admission ---##\n",
      "**Clinical Case Summary:**\n",
      "\n",
      "**Patient Profile:**\n",
      "- Age: 65 years\n",
      "- Gender: Male\n",
      "\n",
      "**Chief Complaint:**\n",
      "- Dyspnea (shortness of breath)\n",
      "\n",
      "**History of Present Illness:**\n",
      "The patient was admitted to the hospital with complaints of increasing dyspnea. The onset, duration, and severity of symptoms, as well as any associated symptoms (e.g., cough, chest pain, wheezing), should be further evaluated. \n",
      "\n",
      "**Past Medical History:**\n",
      "A thorough review of the patient's medical history is essential, including any history of respiratory conditions (e.g., COPD, asthma), cardiovascular diseases (e.g., heart failure, ischemic heart disease), or other relevant comorbidities.\n",
      "\n",
      "**Medications:**\n",
      "A list of current medications, including any recent changes, should be obtained to assess for potential drug-related causes of dyspnea.\n",
      "\n",
      "**Social History:**\n",
      "Information regarding smoking status, occupational exposures, and living conditions may provide insight into potential environmental factors contributing to the patient's condition.\n",
      "\n",
      "**Physical Examination:**\n",
      "A comprehensive physical examination should be performed, focusing on respiratory and cardiovascular systems. Key findings may include respiratory rate, oxygen saturation, lung auscultation, and signs of respiratory distress.\n",
      "\n",
      "**Diagnostic Workup:**\n",
      "Initial diagnostic tests may include:\n",
      "- Chest X-ray to evaluate for pneumonia, heart failure, or other pulmonary conditions.\n",
      "- Complete blood count (CBC) and basic metabolic panel (BMP) to assess for infection or electrolyte imbalances.\n",
      "- Arterial blood gas (ABG) analysis to evaluate oxygenation and acid-base status.\n",
      "- Electrocardiogram (ECG) to rule out cardiac causes of dyspnea.\n",
      "\n",
      "**Assessment and Plan:**\n",
      "Based on the findings from the history, physical examination, and diagnostic tests, a differential diagnosis should be established. Management may include supplemental oxygen, bronchodilators, diuretics (if heart failure is suspected), or other appropriate interventions based on the underlying cause of dyspnea.\n",
      "\n",
      "**Follow-Up:**\n",
      "Close monitoring of the patient's respiratory status and response to treatment is essential. Further investigations may be warranted based on initial findings.\n",
      "\n",
      "**Conclusion:**\n",
      "This 65-year-old male presents with dyspnea, necessitating a thorough evaluation to determine the underlying cause and initiate appropriate management.\n",
      "\n",
      " --------------------\n",
      "##--- Step 2: Vitals ---##\n",
      "**Clinical Case Summary (Updated):**\n",
      "\n",
      "**Patient Profile:**\n",
      "- Age: 65 years\n",
      "- Gender: Male\n",
      "\n",
      "**Chief Complaint:**\n",
      "- Dyspnea (shortness of breath)\n",
      "\n",
      "**Vital Signs:**\n",
      "- Blood Pressure: 140/90 mmHg\n",
      "- Heart Rate: 110 bpm (tachycardia)\n",
      "- Oxygen Saturation: 88% on room air (hypoxemia)\n",
      "\n",
      "**History of Present Illness:**\n",
      "The patient presents with increasing dyspnea. The vital signs indicate elevated blood pressure and heart rate, along with significant hypoxemia. Further details regarding the onset, duration, and associated symptoms (e.g., cough, chest pain, wheezing) are necessary for a comprehensive assessment.\n",
      "\n",
      "**Past Medical History:**\n",
      "A detailed review of the patient's medical history is crucial, particularly for any history of respiratory or cardiovascular conditions, such as chronic obstructive pulmonary disease (COPD), heart failure, or ischemic heart disease.\n",
      "\n",
      "**Medications:**\n",
      "A complete list of current medications should be obtained to identify any potential contributors to the patient's symptoms.\n",
      "\n",
      "**Social History:**\n",
      "Information regarding smoking status, occupational exposures, and living conditions should be gathered to assess environmental factors that may impact respiratory health.\n",
      "\n",
      "**Physical Examination:**\n",
      "A thorough physical examination is warranted, focusing on the respiratory and cardiovascular systems. Key findings may include:\n",
      "- Respiratory rate and effort\n",
      "- Lung auscultation for wheezing, crackles, or diminished breath sounds\n",
      "- Signs of respiratory distress (e.g., use of accessory muscles, cyanosis)\n",
      "\n",
      "**Diagnostic Workup:**\n",
      "Initial diagnostic tests should include:\n",
      "- Chest X-ray to evaluate for pneumonia, heart failure, or other pulmonary conditions.\n",
      "- Complete blood count (CBC) and basic metabolic panel (BMP) to assess for infection or electrolyte imbalances.\n",
      "- Arterial blood gas (ABG) analysis to evaluate oxygenation and acid-base status.\n",
      "- Electrocardiogram (ECG) to rule out cardiac causes of dyspnea.\n",
      "\n",
      "**Assessment and Plan:**\n",
      "The patient's vital signs indicate significant hypoxemia (SpO2 88%) and tachycardia (HR 110 bpm), which may suggest acute respiratory distress or cardiac involvement. Immediate management should include:\n",
      "- Supplemental oxygen to improve oxygen saturation.\n",
      "- Consideration of bronchodilators if bronchospasm is suspected.\n",
      "- Diuretics if heart failure is a concern, pending further evaluation.\n",
      "- Continuous monitoring of vital signs and respiratory status.\n",
      "\n",
      "**Follow-Up:**\n",
      "Close monitoring is essential, with reassessment of oxygenation and response to treatment. Further investigations may be warranted based on initial findings, and consultation with specialists (e.g., pulmonology or cardiology) may be considered.\n",
      "\n",
      "**Conclusion:**\n",
      "This 65-year-old male presents with dyspnea, tachycardia, and hypoxemia, necessitating urgent evaluation and management to determine the underlying cause and initiate appropriate treatment.\n",
      "\n",
      " --------------------\n",
      "##--- Step 3: Retrieving From Memory ---##\n",
      "The patient is a 65-year-old male presenting with dyspnea, tachycardia (HR 110 bpm), and hypoxemia (SpO2 88% on room air), indicating a potential acute respiratory or cardiac event requiring immediate evaluation and intervention.\n"
     ]
    }
   ],
   "source": [
    "print('##--- Step 1: Admission ---##')\n",
    "print(update_and_chat('Patient: 65-year-old male admitted with dyspnea.'))\n",
    "print('\\n', '-' * 20)\n",
    "# Model now knows age. gender and symptom.\n",
    "\n",
    "print('##--- Step 2: Vitals ---##')\n",
    "print(update_and_chat('BP: 140/90, HR: 110. SpO2: 88% on room air.'))\n",
    "print('\\n', '-' * 20)\n",
    "# Model now knows vitals AND the previous admission info.\n",
    "\n",
    "print('##--- Step 3: Retrieving From Memory ---##')\n",
    "print(update_and_chat('Based on the vitals and symptoms, generate a 1-sentence triage assessment.'))\n",
    "# We ask a question that requires knowledge from Step 1 and Step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d89da71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##--- Under the Hood: The Accumulated Context ---##\n",
      "[SYSTEM]: You are a Clinical Case Summarizer. Maintain a professional tone....\n",
      "[USER]: Patient: 65-year-old male admitted with dyspnea....\n",
      "[ASSISTANT]: **Clinical Case Summary:**\n",
      "\n",
      "**Patient Profile:**\n",
      "- Age: 65 years\n",
      "- Gender: Male\n",
      "\n",
      "**Chief Complaint:*...\n",
      "[USER]: BP: 140/90, HR: 110. SpO2: 88% on room air....\n",
      "[ASSISTANT]: **Clinical Case Summary (Updated):**\n",
      "\n",
      "**Patient Profile:**\n",
      "- Age: 65 years\n",
      "- Gender: Male\n",
      "\n",
      "**Chief C...\n",
      "[USER]: Based on the vitals and symptoms, generate a 1-sentence triage assessment....\n",
      "[ASSISTANT]: The patient is a 65-year-old male presenting with dyspnea, tachycardia (HR 110 bpm), and hypoxemia (...\n"
     ]
    }
   ],
   "source": [
    "# Inspecting the Memory\n",
    "print('##--- Under the Hood: The Accumulated Context ---##')\n",
    "\n",
    "for msg in messages_buffer:\n",
    "    print(f'[{msg[\"role\"].upper()}]: {msg[\"content\"][:100]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cfe5c9",
   "metadata": {},
   "source": [
    "# 7. Production Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ab2cf",
   "metadata": {},
   "source": [
    "While functional scripts are sufficient for ad-hoc analysis, production systems (e.g., Streamlit Dashboards, FastAPI backends) require a robust **Object-Oriented** architecture.\n",
    "\n",
    "We encapsulate the logic into a `ClinicalChatbot` class to solve three engineering challenges:\n",
    "\n",
    "1.  **Encapsulation:** Bundling configuration (Model ID, Temperature) with State (Conversation History).\n",
    "2.  **Scalability:** Allowing multiple independent agent instances (e.g., one for Oncology, one for Cardiology) to run simultaneously without state collision.\n",
    "3.  **Resilience:** Centralized error handling and token usage monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "013ffae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class ClinicalChatBot:\n",
    "\n",
    "    '''\n",
    "    A production-ready wrapper for OpenAI API interaction, designed for\n",
    "    maintaining conversational state in clinical decision support workflow.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 system_role: str,\n",
    "                 model: str = 'gpt-4o-mini',\n",
    "                 temperature: float = 0.0):\n",
    "        \n",
    "        ''' \n",
    "        Process:\n",
    "            Initializes the chatbot with a specific persona and configuration.\n",
    "        Args:\n",
    "            system (str): The meta-prompt defining behaviour and guardrails.\n",
    "            model (str): Target inference engine. Defaults to cost-efficient \"gpt-4o-mini\"\n",
    "            temperature (float): Determinism factor.\n",
    "        '''\n",
    "\n",
    "        self.client = OpenAI()\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.system_role = system_role\n",
    "        self.history: List[Dict[str, str]] = []\n",
    "\n",
    "        # Initialize memory\n",
    "        self.reset_memory()\n",
    "\n",
    "    # Method: Reset memory \n",
    "    def reset_memory(self):\n",
    "        '''\n",
    "        Resets conversation history to the initial system state.\n",
    "        '''\n",
    "        self.history = [{'role': 'system', 'content': self.system_role}]\n",
    "\n",
    "    # Method: Chat\n",
    "    def chat(self,\n",
    "                user_input: str,\n",
    "                verbose: bool = False) -> str:\n",
    "        \n",
    "        '''\n",
    "        Process:\n",
    "            Receives and processes a user query, updates internal state,\n",
    "            and returns the model response\n",
    "        Args:\n",
    "            user_input (str): The clinical query or data payload.\n",
    "            verbose (bool): If True, prints token usage metrics.\n",
    "        Returns:\n",
    "            str: The generated response text.\n",
    "        '''\n",
    "\n",
    "        # 1. State Update: Add user input\n",
    "        self.history.append({'role': 'user', 'content': user_input})\n",
    "\n",
    "        try:\n",
    "            # 2. API Execution\n",
    "            response = client.chat.completions.create(\n",
    "                model = self.model,\n",
    "                temperature = self.temperature,\n",
    "                messages = self.history\n",
    "            )\n",
    "\n",
    "            # 3. Extraction\n",
    "            answer = response.choices[0].message.content\n",
    "\n",
    "            # 4. State Update: Add Assistance Response\n",
    "            self.history.append({'role': 'assistant', 'content': answer})\n",
    "\n",
    "            # 5. Telemetry (Optional)\n",
    "            if verbose:\n",
    "                usage = response.usage\n",
    "                print(f'Metrics | In: {usage.prompt_tokens} | Out: {usage.completion_tokens}')\n",
    "\n",
    "            return answer\n",
    "        \n",
    "        except Exception as e:\n",
    "            # In production this should log to a monitoring service\n",
    "            return f'API ERROR: {str(e)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eeabc018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Implementation ----\n",
    "\n",
    "# 1. Instantiate an Oncology Specialist ChatBot\n",
    "system_role_definition = ''' \n",
    "You are an Oncology Assistant.\n",
    "Summarize patient history in 3 sentences, focusing on tumor markers and progression.\n",
    "'''\n",
    "\n",
    "oncology_bot = ClinicalChatBot(\n",
    "    system_role = system_role_definition,\n",
    "    temperature = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86315f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##--- Oncology Session Started ---##\n",
      "User: Patient 45M, Stage III NSCLC. PD-L1 expression >50%.\n",
      "Metrics | In: 52 | Out: 60\n",
      "ChatBot: The patient is a 45-year-old male diagnosed with Stage III non-small cell lung cancer (NSCLC). Tumor markers indicate a PD-L1 expression greater than 50%, suggesting a potential for immunotherapy treatment options. There is no additional information provided regarding tumor progression or response to previous treatments.\n"
     ]
    }
   ],
   "source": [
    "# 2. Simulate a Multi-Turn Conversation\n",
    "print('##--- Oncology Session Started ---##')\n",
    "\n",
    "# Turn 1. Providing Context\n",
    "input_1 = 'Patient 45M, Stage III NSCLC. PD-L1 expression >50%.'\n",
    "print(f'User: {input_1}')\n",
    "response = oncology_bot.chat(input_1, verbose = True)\n",
    "print(f'ChatBot: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f876c51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Based on the expression level, what is the first-line therapy?\n",
      "Metrics | In: 133 | Out: 68\n",
      "ChatBot: Based on the PD-L1 expression level greater than 50%, the first-line therapy for this patient with Stage III non-small cell lung cancer (NSCLC) would typically be pembrolizumab (Keytruda) in combination with chemotherapy. This approach leverages the high PD-L1 expression to enhance the immune response against the tumor.\n"
     ]
    }
   ],
   "source": [
    "# Turn 2. Asking a Recommendation\n",
    "input_2 = 'Based on the expression level, what is the first-line therapy?'\n",
    "print(f'User: {input_2}')\n",
    "response_2 = oncology_bot.chat(input_2, verbose = True)\n",
    "print(f'ChatBot: {response_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33a1fa5",
   "metadata": {},
   "source": [
    "# 8. Conclusion & Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f29c34",
   "metadata": {},
   "source": [
    "We have successfully evolved from simple, functional API scripts to a **Production-Grade OOP Architecture**.\n",
    "\n",
    "**Summary of Achievements:**\n",
    "1.  **Secure Architecture:** Implemented environment-based authentication to prevent credential leakage.\n",
    "2.  **Cost Engineering:** Established token monitoring and optimization patterns.\n",
    "3.  **State Management:** Built a robust memory system (`ClinicalChatbot`) capable of handling multi-turn clinical conversations.\n",
    "4.  **Reproducibility:** Demonstrated control over stochasticity using temperature parameters for GxP compliance.\n",
    "\n",
    "**Limitations:**\n",
    "Currently, our chatbot relies solely on its pre-trained \"parametric memory\" (Internet knowledge up to the training cutoff). It cannot yet access private patient records or external medical databases.\n",
    "\n",
    "**Potential Improvements:**\n",
    "By building a **RAG (Retrieval-Augmented Generation)** system, giving our ChatBot (Agent) access to proprietary external documents (PDFs, Guidelines) with / without fine-tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-playbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
